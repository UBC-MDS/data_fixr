[
  {
    "objectID": "reference/correlation_report.html",
    "href": "reference/correlation_report.html",
    "title": "correlation_report",
    "section": "",
    "text": "correlation_report\n\n\n\n\n\nName\nDescription\n\n\n\n\ncorrelation_report\nCompute pairwise correlations between numeric columns of a DataFrame.\n\n\n\n\n\ncorrelation_report.correlation_report(df, method='pearson')\nCompute pairwise correlations between numeric columns of a DataFrame.\nThis function is intended for exploratory data analysis diagnostics without plotting. It computes pairwise correlations between numeric features and returns a long-format report table, where each row corresponds to a unique feature pair.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput pandas DataFrame containing the data to analyze.\nrequired\n\n\nmethod\nstr\nCorrelation method to use. Supported values are: - “pearson”: linear correlation. - “spearman”: rank-based correlation. - “kendall”: rank-based correlation.\n'pearson'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nA long-format correlation report with the following columns: - feature_1: name of the first feature - feature_2: name of the second feature - correlation: correlation value - abs_correlation: absolute value of the correlation Each row represents a unique pair of numeric features. Self-correlations and duplicate symmetric pairs are excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the supported correlation methods. If fewer than two numeric columns are available for correlation.\n\n\n\n\n\n\n\nOnly numeric columns are considered.\nMissing values are handled according to pandas’ correlation behavior.\nThis function does not generate plots or files.\nThe output is intended to be machine-readable and suitable for use in automated analysis or reporting pipelines.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"age\": [20, 30, 40],\n...     \"income\": [40000, 60000, 80000],\n...     \"score\": [3, 2, 1],\n... })\n&gt;&gt;&gt; correlation_report(df, method=\"pearson\")\n  feature_1 feature_2  correlation  abs_correlation\n0       age    income          1.0              1.0\n1       age     score         -1.0              1.0\n2    income     score         -1.0              1.0",
    "crumbs": [
      "Reference",
      "Some functions",
      "correlation_report"
    ]
  },
  {
    "objectID": "reference/correlation_report.html#functions",
    "href": "reference/correlation_report.html#functions",
    "title": "correlation_report",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncorrelation_report\nCompute pairwise correlations between numeric columns of a DataFrame.\n\n\n\n\n\ncorrelation_report.correlation_report(df, method='pearson')\nCompute pairwise correlations between numeric columns of a DataFrame.\nThis function is intended for exploratory data analysis diagnostics without plotting. It computes pairwise correlations between numeric features and returns a long-format report table, where each row corresponds to a unique feature pair.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput pandas DataFrame containing the data to analyze.\nrequired\n\n\nmethod\nstr\nCorrelation method to use. Supported values are: - “pearson”: linear correlation. - “spearman”: rank-based correlation. - “kendall”: rank-based correlation.\n'pearson'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nA long-format correlation report with the following columns: - feature_1: name of the first feature - feature_2: name of the second feature - correlation: correlation value - abs_correlation: absolute value of the correlation Each row represents a unique pair of numeric features. Self-correlations and duplicate symmetric pairs are excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the supported correlation methods. If fewer than two numeric columns are available for correlation.\n\n\n\n\n\n\n\nOnly numeric columns are considered.\nMissing values are handled according to pandas’ correlation behavior.\nThis function does not generate plots or files.\nThe output is intended to be machine-readable and suitable for use in automated analysis or reporting pipelines.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"age\": [20, 30, 40],\n...     \"income\": [40000, 60000, 80000],\n...     \"score\": [3, 2, 1],\n... })\n&gt;&gt;&gt; correlation_report(df, method=\"pearson\")\n  feature_1 feature_2  correlation  abs_correlation\n0       age    income          1.0              1.0\n1       age     score         -1.0              1.0\n2    income     score         -1.0              1.0",
    "crumbs": [
      "Reference",
      "Some functions",
      "correlation_report"
    ]
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Functions to inspect docstrings.\n\n\n\ncorrelation_report\n\n\n\ndetect_anomalies\n\n\n\nmissing_values\n\n\n\nremove_duplicates",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#some-functions",
    "href": "reference/index.html#some-functions",
    "title": "Function reference",
    "section": "",
    "text": "Functions to inspect docstrings.\n\n\n\ncorrelation_report\n\n\n\ndetect_anomalies\n\n\n\nmissing_values\n\n\n\nremove_duplicates",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/missing_values.html",
    "href": "reference/missing_values.html",
    "title": "missing_values",
    "section": "",
    "text": "missing_values\n\n\n\n\n\nName\nDescription\n\n\n\n\nmissing_values\nThis function fills missing values (NaN) in a pandas DataFrame using\n\n\n\n\n\nmissing_values.missing_values(df, method)\nThis function fills missing values (NaN) in a pandas DataFrame using column-appropriate imputation strategies.\nThis function imputes missing values in both numeric and categorical columns. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation.\nMissing values can distort statistical analyses and machine learning models. This function provides common strategies for imputing missing values depending on the nature of the data distribution.\nThe function identifies numeric and non-numeric columns and applies imputation independently to each column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing missing values to be imputed.\nrequired\n\n\nmethod\nstr\nThe imputation method to use for numeric columns. Valid options are: - ‘mean’ : Replace NaN with column mean (suitable for symmetric data) - ‘median’ : Replace NaN with column median (robust to outliers) - ‘mode’ : Replace NaN with column mode Categorical (non-numeric) columns always use mode imputation regardless of the selected method.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple of (pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame with missing values filled in both numeric and categorical (non-numeric) columns. filled_percentage : float The percentage of total DataFrame values that were originally missing and have been filled, calculated as: (number of filled values / number of total values) * 100. Columns containing only NaN values are left unchanged and do not contribute any filled values to this percentage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the 3 supported numeric options.\n\n\n\n\n\n\n\nNumeric columns are imputed using the specified method.\nCategorical (non-numeric) columns are imputed using mode.\nImputation is applied column-wise.\nColumns containing all NaN values are left unchanged and do not affect the filled percentage.\nIf multiple modes exist (for both numeric and categorical columns), the first mode returned by pandas is used.\nThe original DataFrame is not modified; a copy is returned.\nThe filled percentage includes values filled in both numeric and categorical (non-numeric) columns.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'age': [25, 30, np.nan, 28],\n...     'income': [50000, np.nan, 52000, np.nan],\n...     'city': ['A', 'B', np.nan, 'B']\n... })\n&gt;&gt;&gt; result_df, filled_percentage = missing_values(df, method='median')\n&gt;&gt;&gt; print(result_df)\n    age   income city\n0  25.0  50000.0    A\n1  30.0  51000.0    B\n2  28.0  52000.0    B\n3  28.0  51000.0    B\n&gt;&gt;&gt; print(f\"{filled_percentage:.1f}% of values were filled.\")\n33.3% of values were filled.",
    "crumbs": [
      "Reference",
      "Some functions",
      "missing_values"
    ]
  },
  {
    "objectID": "reference/missing_values.html#functions",
    "href": "reference/missing_values.html#functions",
    "title": "missing_values",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmissing_values\nThis function fills missing values (NaN) in a pandas DataFrame using\n\n\n\n\n\nmissing_values.missing_values(df, method)\nThis function fills missing values (NaN) in a pandas DataFrame using column-appropriate imputation strategies.\nThis function imputes missing values in both numeric and categorical columns. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation.\nMissing values can distort statistical analyses and machine learning models. This function provides common strategies for imputing missing values depending on the nature of the data distribution.\nThe function identifies numeric and non-numeric columns and applies imputation independently to each column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing missing values to be imputed.\nrequired\n\n\nmethod\nstr\nThe imputation method to use for numeric columns. Valid options are: - ‘mean’ : Replace NaN with column mean (suitable for symmetric data) - ‘median’ : Replace NaN with column median (robust to outliers) - ‘mode’ : Replace NaN with column mode Categorical (non-numeric) columns always use mode imputation regardless of the selected method.\nrequired\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple of (pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame with missing values filled in both numeric and categorical (non-numeric) columns. filled_percentage : float The percentage of total DataFrame values that were originally missing and have been filled, calculated as: (number of filled values / number of total values) * 100. Columns containing only NaN values are left unchanged and do not contribute any filled values to this percentage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the 3 supported numeric options.\n\n\n\n\n\n\n\nNumeric columns are imputed using the specified method.\nCategorical (non-numeric) columns are imputed using mode.\nImputation is applied column-wise.\nColumns containing all NaN values are left unchanged and do not affect the filled percentage.\nIf multiple modes exist (for both numeric and categorical columns), the first mode returned by pandas is used.\nThe original DataFrame is not modified; a copy is returned.\nThe filled percentage includes values filled in both numeric and categorical (non-numeric) columns.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'age': [25, 30, np.nan, 28],\n...     'income': [50000, np.nan, 52000, np.nan],\n...     'city': ['A', 'B', np.nan, 'B']\n... })\n&gt;&gt;&gt; result_df, filled_percentage = missing_values(df, method='median')\n&gt;&gt;&gt; print(result_df)\n    age   income city\n0  25.0  50000.0    A\n1  30.0  51000.0    B\n2  28.0  52000.0    B\n3  28.0  51000.0    B\n&gt;&gt;&gt; print(f\"{filled_percentage:.1f}% of values were filled.\")\n33.3% of values were filled.",
    "crumbs": [
      "Reference",
      "Some functions",
      "missing_values"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "Contributions of all kinds are welcome for the data_fixr package and are greatly appreciated! Every little bit helps, and credit will always be given.\n\n\nThis project follows a Github Flow Workflow.\n\nThe main branch always contains stable, production-ready code\nDirect commits to the ‘main’ branch are prohibited.\nAll work should be conducted on short-lived branches created from ‘main’.\nBranches should be named using appropriate prefixes such as ‘feat-’ or ‘fix-’.\nChanges must be proposed via pull request to the main branch.\nEach Pull request must be reviewed by at least one other team member and granted approval before merging.\n\n\n\n\nYou can contribute in many ways, for example:\n\nReport bugs\nFix Bugs\nImplement Features\nWrite Documentation\nSubmit Feedback\n\n\n\nReport bugs at https://github.com/UBC-MDS/data_fixr/issues.\nIf you are reporting a bug, please follow the template guidelines. The more detailed your report, the easier and thus faster we can help you.\n\n\n\nLook through the GitHub issues for bugs. Anything labelled with bug and help wanted is open to whoever wants to implement it. When you decide to work on such an issue, please assign yourself to it and add a comment that you’ll be working on that, too. If you see another issue without the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\nLook through the GitHub issues for features. Anything labelled with enhancement and help wanted is open to whoever wants to implement it. As for fixing bugs, please assign yourself to the issue and add a comment that you’ll be working on that, too. If another enhancement catches your fancy, but it doesn’t have the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\ndata_fixr could always use more documentation, whether as part of the official documentation, in docstrings, or even on the web in blog posts, articles, and such. Just open an issue to let us know what you will be working on so that we can provide you with guidance.\n\n\n\nThe best way to send feedback is to file an issue at https://github.com/UBC-MDS/data_fixr/issues. If your feedback fits the format of one of the issue templates, please use that. Remember that this is a volunteer-driven project and everybody has limited time.\n\n\n\n\nReady to contribute? Here’s how to set up data_fixr for local development.\n\nFork the https://github.com/UBC-MDS/data_fixr repository on GitHub.\nClone your fork locally (if you want to work locally)\ngit clone git@github.com:your_name_here/data_fixr.git\nInstall hatch.\nCreate a branch for local development using the default branch (typically main) as a starting point. Use fix or feat as a prefix for your branch name.\ngit checkout main\ngit checkout -b fix-name-of-your-bugfix\nNow you can make your changes locally.\nWhen you’re done making changes, apply the quality assurance tools and check that your changes pass our test suite. This is all included with Hatch.\nhatch run test:run\nCommit your changes and push your branch to GitHub. Please use semantic commit messages.\ngit add .\ngit commit -m \"fix: summarize your changes\"\ngit push -u origin fix-name-of-your-bugfix\nOpen the link displayed in the message when pushing your new branch in order to submit a pull request.\n\n\n\n\nBefore you submit a pull request, check that it meets these guidelines:\n\nThe pull request should include tests.\nIf the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring.\nYour pull request will automatically be checked by the full test suite. It needs to pass all of them before it can be considered for merging.\n\n\n\n\nPlease note that the data_fixr package is released with a Code of Conduct. By contributing to this project you agree to abide by its terms."
  },
  {
    "objectID": "CONTRIBUTING.html#branching-and-workflow",
    "href": "CONTRIBUTING.html#branching-and-workflow",
    "title": "Contributing",
    "section": "",
    "text": "This project follows a Github Flow Workflow.\n\nThe main branch always contains stable, production-ready code\nDirect commits to the ‘main’ branch are prohibited.\nAll work should be conducted on short-lived branches created from ‘main’.\nBranches should be named using appropriate prefixes such as ‘feat-’ or ‘fix-’.\nChanges must be proposed via pull request to the main branch.\nEach Pull request must be reviewed by at least one other team member and granted approval before merging."
  },
  {
    "objectID": "CONTRIBUTING.html#example-contributions",
    "href": "CONTRIBUTING.html#example-contributions",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute in many ways, for example:\n\nReport bugs\nFix Bugs\nImplement Features\nWrite Documentation\nSubmit Feedback\n\n\n\nReport bugs at https://github.com/UBC-MDS/data_fixr/issues.\nIf you are reporting a bug, please follow the template guidelines. The more detailed your report, the easier and thus faster we can help you.\n\n\n\nLook through the GitHub issues for bugs. Anything labelled with bug and help wanted is open to whoever wants to implement it. When you decide to work on such an issue, please assign yourself to it and add a comment that you’ll be working on that, too. If you see another issue without the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\nLook through the GitHub issues for features. Anything labelled with enhancement and help wanted is open to whoever wants to implement it. As for fixing bugs, please assign yourself to the issue and add a comment that you’ll be working on that, too. If another enhancement catches your fancy, but it doesn’t have the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\ndata_fixr could always use more documentation, whether as part of the official documentation, in docstrings, or even on the web in blog posts, articles, and such. Just open an issue to let us know what you will be working on so that we can provide you with guidance.\n\n\n\nThe best way to send feedback is to file an issue at https://github.com/UBC-MDS/data_fixr/issues. If your feedback fits the format of one of the issue templates, please use that. Remember that this is a volunteer-driven project and everybody has limited time."
  },
  {
    "objectID": "CONTRIBUTING.html#get-started",
    "href": "CONTRIBUTING.html#get-started",
    "title": "Contributing",
    "section": "",
    "text": "Ready to contribute? Here’s how to set up data_fixr for local development.\n\nFork the https://github.com/UBC-MDS/data_fixr repository on GitHub.\nClone your fork locally (if you want to work locally)\ngit clone git@github.com:your_name_here/data_fixr.git\nInstall hatch.\nCreate a branch for local development using the default branch (typically main) as a starting point. Use fix or feat as a prefix for your branch name.\ngit checkout main\ngit checkout -b fix-name-of-your-bugfix\nNow you can make your changes locally.\nWhen you’re done making changes, apply the quality assurance tools and check that your changes pass our test suite. This is all included with Hatch.\nhatch run test:run\nCommit your changes and push your branch to GitHub. Please use semantic commit messages.\ngit add .\ngit commit -m \"fix: summarize your changes\"\ngit push -u origin fix-name-of-your-bugfix\nOpen the link displayed in the message when pushing your new branch in order to submit a pull request."
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-guidelines",
    "href": "CONTRIBUTING.html#pull-request-guidelines",
    "title": "Contributing",
    "section": "",
    "text": "Before you submit a pull request, check that it meets these guidelines:\n\nThe pull request should include tests.\nIf the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring.\nYour pull request will automatically be checked by the full test suite. It needs to pass all of them before it can be considered for merging."
  },
  {
    "objectID": "CONTRIBUTING.html#code-of-conduct",
    "href": "CONTRIBUTING.html#code-of-conduct",
    "title": "Contributing",
    "section": "",
    "text": "Please note that the data_fixr package is released with a Code of Conduct. By contributing to this project you agree to abide by its terms."
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Welcome to data_fixr’s Documentation ’",
    "section": "",
    "text": ":maxdepth: 2 :hidden: :caption: Contents:\nHome \n\nThis is the landing page of your docs. you can update it as you’d like to. This documentation example uses myst markdown as the primary documentation syntax.\n:::{button-link} https://www.pyopensci.org/python-package-guide/documentation/hosting-tools/myst-markdown-rst-doc-syntax.html :color: primary :class: sd-rounded-pill float-left\nLearn more about myst in our pyOpenSci packaging guide.\n:::\nMyst is a version of markdown that has more formatting flexibility. This is what a sphinx directive looks like using myst markdown formatting:\n:::{toctree}\n:maxdepth: 2\n:caption: Contents:\n:::\nIf you see syntax like the syntax below, you are looking at rst.\n.. toctree::\n   :maxdepth: 2\n   :caption: Contents:\n\n\n\n\nCopyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe .\nFree software distributed under the MIT License."
  },
  {
    "objectID": "docs/index.html#overview",
    "href": "docs/index.html#overview",
    "title": "Welcome to data_fixr’s Documentation ’",
    "section": "",
    "text": ":maxdepth: 2 :hidden: :caption: Contents:\nHome \n\nThis is the landing page of your docs. you can update it as you’d like to. This documentation example uses myst markdown as the primary documentation syntax.\n:::{button-link} https://www.pyopensci.org/python-package-guide/documentation/hosting-tools/myst-markdown-rst-doc-syntax.html :color: primary :class: sd-rounded-pill float-left\nLearn more about myst in our pyOpenSci packaging guide.\n:::\nMyst is a version of markdown that has more formatting flexibility. This is what a sphinx directive looks like using myst markdown formatting:\n:::{toctree}\n:maxdepth: 2\n:caption: Contents:\n:::\nIf you see syntax like the syntax below, you are looking at rst.\n.. toctree::\n   :maxdepth: 2\n   :caption: Contents:"
  },
  {
    "objectID": "docs/index.html#copyright",
    "href": "docs/index.html#copyright",
    "title": "Welcome to data_fixr’s Documentation ’",
    "section": "",
    "text": "Copyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe .\nFree software distributed under the MIT License."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct aims to create a welcoming, inclusive and harassment-free environment for all contributors. All participants are expected to treat each other with respect, and abusive or inappropriate behavior will not be tolerated.\n\n\nAs contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, submitting pull requests and other activities.\nWe are committed to making participation in this project a harassment-free experience for everybody, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion or nationality.\n\n\n\nTo foster an open and welcoming community, all contributors are expected to:\n\nUse welcoming and inclusive language in all interactions\nBe considerate of differing viewpoints and experiences\nAccept constructive criticism\nFocus on what is best for the community\nWork constructively with other contributors and be open to feedback\n\n\n\n\nExamples of unacceptable behavior by contributors include:\n\nThe use of sexualized language or imagery\nPersonal attacks, trolling or insulting/derogatory comments\nPublic or private harassment\nPublishing others’ private information, such as physical or electronic addresses, without explicit permission\nOther conduct which could reasonably be considered inappropriate, unethical or unprofessional\n\n\n\nProject maintainers have the right and responsibility to review and, if necessary, remove, edit or reject comments, code, issues and other contributions that are not aligned with this Code of Conduct.\nConsequences of violations may include:\n\nA written warning regarding their behavior\nTemporary restriction from interacting with the project for a specified period\nPermanent removal from the project and all associated spaces\n\nThe severity of the consequences will depend on the nature and frequency of the violation.\n\n\n\nInstances of abusive, harassing or otherwise unacceptable behavior may be reported by contacting any member of the project team directly or by e-mail: asrivas1@student.ubc.ca.\nWhen reporting an incident, please include your name and contact information, a description of the incident and any additional context that might be helpful. All complaints will be reviewed and investigated promptly and will result in a response that is deemed appropriate to the circumstances. All reports will be kept strictly confidential.\n\n\n\n\nThis Code of Conduct is adapted from the Pandas Code of Conduct, which was adapted from the Contributor Covenant, version 1.3.0, available at https://www.contributor-covenant.org/version/1/3/0/code-of-conduct/ and the Swift Code of Conduct."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#statement-on-diversity-and-inclusion",
    "href": "CODE_OF_CONDUCT.html#statement-on-diversity-and-inclusion",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, submitting pull requests and other activities.\nWe are committed to making participation in this project a harassment-free experience for everybody, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion or nationality."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#expected-behavior",
    "href": "CODE_OF_CONDUCT.html#expected-behavior",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "To foster an open and welcoming community, all contributors are expected to:\n\nUse welcoming and inclusive language in all interactions\nBe considerate of differing viewpoints and experiences\nAccept constructive criticism\nFocus on what is best for the community\nWork constructively with other contributors and be open to feedback"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "href": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Examples of unacceptable behavior by contributors include:\n\nThe use of sexualized language or imagery\nPersonal attacks, trolling or insulting/derogatory comments\nPublic or private harassment\nPublishing others’ private information, such as physical or electronic addresses, without explicit permission\nOther conduct which could reasonably be considered inappropriate, unethical or unprofessional\n\n\n\nProject maintainers have the right and responsibility to review and, if necessary, remove, edit or reject comments, code, issues and other contributions that are not aligned with this Code of Conduct.\nConsequences of violations may include:\n\nA written warning regarding their behavior\nTemporary restriction from interacting with the project for a specified period\nPermanent removal from the project and all associated spaces\n\nThe severity of the consequences will depend on the nature and frequency of the violation.\n\n\n\nInstances of abusive, harassing or otherwise unacceptable behavior may be reported by contacting any member of the project team directly or by e-mail: asrivas1@student.ubc.ca.\nWhen reporting an incident, please include your name and contact information, a description of the incident and any additional context that might be helpful. All complaints will be reviewed and investigated promptly and will result in a response that is deemed appropriate to the circumstances. All reports will be kept strictly confidential."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Pandas Code of Conduct, which was adapted from the Contributor Covenant, version 1.3.0, available at https://www.contributor-covenant.org/version/1/3/0/code-of-conduct/ and the Swift Code of Conduct."
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\n\n\n\nUpcoming features and fixes\n\n\n\n\n\n\n\nFunction specifications and documentation for correlation_report() (#15)\nFunction specifications and documentation for detect_anomalies() (#16)\nFunction specifications and documentation for remove_duplicates() (#21)\nFunction specifications and documentation for missing_values() (#22)\nSupport for handling missing values in categorical columns (#17)\n\n\n\n\n\nUpdated README.md with project details and usage information (#24)\nUpdated CONTRIBUTING.md with contribution guidelines (#18)\nUpdated CODE_OF_CONDUCT.md with code of conduct (#23)\n\n\n\n\n\n\n\n\nImplementation of correlation_report() for computing pairwise correlations (#44)\nImplementation of detect_anomalies() for identifying outliers in data (#42)\nImplementation of remove_duplicates() for duplicate row detection and removal (#45)\nImplementation of missing_values() for handling missing data (#49)\nUnit tests for correlation_report() (#44)\nUnit tests for detect_anomalies() (#42)\nUnit tests for remove_duplicates() (#45)\nUnit tests for missing_values() (#49)\nDependencies added to pyproject.toml (#42)\nEnvironment configuration file environment.yml (#52)\n\n\n\n\n\nUpdated README.md installation instructions for clarity (#51, #52, #53)\n\n\n\n\n\nUpdated CHANGELOG.md to reflect Milestone 1 and 2 progress (#39)\nUpdated __init__.py with correct package imports (#53)"
  },
  {
    "objectID": "CHANGELOG.html#unreleased",
    "href": "CHANGELOG.html#unreleased",
    "title": "Changelog",
    "section": "",
    "text": "Upcoming features and fixes"
  },
  {
    "objectID": "CHANGELOG.html#milestone-1---2026-01-10",
    "href": "CHANGELOG.html#milestone-1---2026-01-10",
    "title": "Changelog",
    "section": "",
    "text": "Function specifications and documentation for correlation_report() (#15)\nFunction specifications and documentation for detect_anomalies() (#16)\nFunction specifications and documentation for remove_duplicates() (#21)\nFunction specifications and documentation for missing_values() (#22)\nSupport for handling missing values in categorical columns (#17)\n\n\n\n\n\nUpdated README.md with project details and usage information (#24)\nUpdated CONTRIBUTING.md with contribution guidelines (#18)\nUpdated CODE_OF_CONDUCT.md with code of conduct (#23)"
  },
  {
    "objectID": "CHANGELOG.html#milestone-2---2026-01-17",
    "href": "CHANGELOG.html#milestone-2---2026-01-17",
    "title": "Changelog",
    "section": "",
    "text": "Implementation of correlation_report() for computing pairwise correlations (#44)\nImplementation of detect_anomalies() for identifying outliers in data (#42)\nImplementation of remove_duplicates() for duplicate row detection and removal (#45)\nImplementation of missing_values() for handling missing data (#49)\nUnit tests for correlation_report() (#44)\nUnit tests for detect_anomalies() (#42)\nUnit tests for remove_duplicates() (#45)\nUnit tests for missing_values() (#49)\nDependencies added to pyproject.toml (#42)\nEnvironment configuration file environment.yml (#52)\n\n\n\n\n\nUpdated README.md installation instructions for clarity (#51, #52, #53)\n\n\n\n\n\nUpdated CHANGELOG.md to reflect Milestone 1 and 2 progress (#39)\nUpdated __init__.py with correct package imports (#53)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Package\n \n\n\nMeta\n\n\n\n\nTODO: the above badges that indicate python version and package version will only work if your package is on PyPI. If you don’t plan to publish to PyPI, you can remove them.\n\n\ndata_fixr is a lightweight Python package designed to support exploratory data analysis and early-stage data cleaning for tabular datasets. The package provides standardized, machine-readable diagnostics and cleaning utilities that help users quickly assess data quality, identify common issues, and prepare datasets for downstream analysis or machine learning workflows. Rather than generating plots or reports, data_fixr focuses on returning clean, structured outputs that can be easily tested, logged, or integrated into automated pipelines.\n\n\n\n\nCorrelation Report: Computes pairwise correlations between numeric columns in a pandas DataFrame and returns a long-format diagnostic table. The output summarizes the strength and direction of relationships between features in a standardized, machine-readable format suitable for exploratory analysis and preprocessing workflows.\nRemove Duplicates: Identifies and removes duplicate rows for a given dataset. Users can specify the retention strategy for handling duplicates and choose whether duplicates are detected using all columns or a specified subset. Optionally, the function can return a useful summary report describing the number of duplicate rows detected and removed.\nDetect Anomalies: This function identifies and flags outliers in numeric columns of a pandas DataFrame using either the Z-score method (for normally distributed data) or the IQR (Interquartile Range) method (for skewed data). It returns a DataFrame with the numeric columns and boolean outlier flags for each numeric column, as well as the overall percentage of outliers detected.\nMissing Values: This function fills missing values (NaN) in both numeric and categorical (non-numeric) columns of a pandas DataFrame. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation. The function returns a DataFrame with missing values filled and the overall percentage of missing values that were imputed across all columns.\n\n\n\n\nThe functionality provided by data_fixr overlaps partially with capabilities available in established Python data analysis libraries, most notably pandas and scikit-learn. Pandas provides low-level methods for computing correlations between numeric variables and for removing duplicate rows, while scikit-learn and related libraries offer utilities for identifying outliers as part of preprocessing or feature selection workflows. However, these tools typically expose such functionality as individual operations without producing standardized, diagnostics-oriented summaries.\nIn particular, while duplicate removal and missing-value imputation functionality already exist in pandas, the corresponding functions in data_fixr extend this behavior by optionally returning structured summary information. The duplicate removal function can return a summary report describing how many duplicate rows were detected and removed, while the missing-value filling function reports the overall percentage of missing values that were imputed across the dataset. Additionally, automated exploratory data analysis tools such as ydata-profiling focus on generating comprehensive visual or HTML reports, whereas data_fixr emphasizes lightweight, modular functions that return machine-readable outputs intended for exploratory diagnostics, reproducible preprocessing pipelines, and downstream machine learning workflows.\n\n\n\nFirst, clone the repository:\n$ git clone https://github.com/UBC-MDS/data_fixr.git\nNavigate into the package directory:\n$ cd data_fixr\n(Optional) To run the package in a clean environment with Python 3.11.\n$ conda env create -f environment.yml\n$ conda activate data_fixr\nInstall the package in editable mode:\n$ pip install -e .\nTo run the tests (developer mode):\n$ pip install -e \".[tests]\"\n$ pytest \nIf you have opted to use the coda environment, deactivate the environment once finished with:\n$ conda deactivate\n\n\n\n\nPython ≥ 3.10\n\n\n\n\n\nApoorva Srivastava\nChikire Aku-Ibe\nNour Shawky\nZain Nofal\n\n\n\n\n\nCopyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe .\nFree software distributed under the MIT License."
  },
  {
    "objectID": "index.html#package-summary",
    "href": "index.html#package-summary",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "data_fixr is a lightweight Python package designed to support exploratory data analysis and early-stage data cleaning for tabular datasets. The package provides standardized, machine-readable diagnostics and cleaning utilities that help users quickly assess data quality, identify common issues, and prepare datasets for downstream analysis or machine learning workflows. Rather than generating plots or reports, data_fixr focuses on returning clean, structured outputs that can be easily tested, logged, or integrated into automated pipelines."
  },
  {
    "objectID": "index.html#functions",
    "href": "index.html#functions",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Correlation Report: Computes pairwise correlations between numeric columns in a pandas DataFrame and returns a long-format diagnostic table. The output summarizes the strength and direction of relationships between features in a standardized, machine-readable format suitable for exploratory analysis and preprocessing workflows.\nRemove Duplicates: Identifies and removes duplicate rows for a given dataset. Users can specify the retention strategy for handling duplicates and choose whether duplicates are detected using all columns or a specified subset. Optionally, the function can return a useful summary report describing the number of duplicate rows detected and removed.\nDetect Anomalies: This function identifies and flags outliers in numeric columns of a pandas DataFrame using either the Z-score method (for normally distributed data) or the IQR (Interquartile Range) method (for skewed data). It returns a DataFrame with the numeric columns and boolean outlier flags for each numeric column, as well as the overall percentage of outliers detected.\nMissing Values: This function fills missing values (NaN) in both numeric and categorical (non-numeric) columns of a pandas DataFrame. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation. The function returns a DataFrame with missing values filled and the overall percentage of missing values that were imputed across all columns."
  },
  {
    "objectID": "index.html#position-in-python-environment",
    "href": "index.html#position-in-python-environment",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "The functionality provided by data_fixr overlaps partially with capabilities available in established Python data analysis libraries, most notably pandas and scikit-learn. Pandas provides low-level methods for computing correlations between numeric variables and for removing duplicate rows, while scikit-learn and related libraries offer utilities for identifying outliers as part of preprocessing or feature selection workflows. However, these tools typically expose such functionality as individual operations without producing standardized, diagnostics-oriented summaries.\nIn particular, while duplicate removal and missing-value imputation functionality already exist in pandas, the corresponding functions in data_fixr extend this behavior by optionally returning structured summary information. The duplicate removal function can return a summary report describing how many duplicate rows were detected and removed, while the missing-value filling function reports the overall percentage of missing values that were imputed across the dataset. Additionally, automated exploratory data analysis tools such as ydata-profiling focus on generating comprehensive visual or HTML reports, whereas data_fixr emphasizes lightweight, modular functions that return machine-readable outputs intended for exploratory diagnostics, reproducible preprocessing pipelines, and downstream machine learning workflows."
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "First, clone the repository:\n$ git clone https://github.com/UBC-MDS/data_fixr.git\nNavigate into the package directory:\n$ cd data_fixr\n(Optional) To run the package in a clean environment with Python 3.11.\n$ conda env create -f environment.yml\n$ conda activate data_fixr\nInstall the package in editable mode:\n$ pip install -e .\nTo run the tests (developer mode):\n$ pip install -e \".[tests]\"\n$ pytest \nIf you have opted to use the coda environment, deactivate the environment once finished with:\n$ conda deactivate"
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Python ≥ 3.10"
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Apoorva Srivastava\nChikire Aku-Ibe\nNour Shawky\nZain Nofal"
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Copyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe .\nFree software distributed under the MIT License."
  },
  {
    "objectID": "DEVELOPMENT.html",
    "href": "DEVELOPMENT.html",
    "title": "Development Guide",
    "section": "",
    "text": "Welcome to your shiny new package. This page will help you get started with using Hatch to manage your package.\nIf you look at your project, you will see that a pyproject.toml file. This file stores both your package configuration and settings for development tools like Hatch that you will use to work on your package.\nThis file is written using a .toml format. You can learn more about toml here. Here’s the TL&DR:\n\nEach [] section in the toml file is called a table.\nYou can nest tables with double brackets like this[[]]\nTables contain information about a element that you want to configure.\n\nWe are using Hatch as the default packaging tool. Hatch allows you to configure and run environments and scripts similar to workflow tools like tox or nox.\nHach, by default, uses virtual environments (venv) to manage environments. But you can configure it to use other environment tools.Read the hatch documentation to learn more about environments.\nFor this template, we have set up Hatch environments for you to use. At the bottom of your pyproject.toml file, notice a hatch environment section that looks like this:\n########################################\n# Hatch Environments\n########################################\nBelow is the Hatch environment to install your package. Notice that it defines pip and twine as two packages that the environment needs.\n[tool.hatch.envs.build]\ndescription = \"\"\"Test the installation the package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\nThe table below defines the scripts that you will run build and check your package.\n[tool.hatch.envs.build.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\ndetached = true\nYou can enter that environment to check it out:\n$ hatch shell build\nIf you run pip list, in the environment, twine will be there:\n$ pip list\nHatch by default, installs your package in editable mode (-e) into its virtual environments. But if detached=True is set, then it will skip installing your package into the virtual enviornment.\n\n\nBelow you see the Hatch environment test table.\ntool.hatch.envs says, “Hey, Hatch, this is the definition for an environment.” test is the name of the environment.\nThe environment below defines the dependencies that Hatch needs to install into the environment named test.\n[tool.hatch.envs.test]\ndescription = \"\"\"Run the test suite.\"\"\"\ndependencies = [\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-raises\",\n    \"pytest-randomly\",\n    \"pytest-xdist\",\n]\nTo enter a Hatch environment use:\nhatch shell environmentname\nSo you can enter the test environment above with:\nhatch shell test\n\n\n\nIf the environment has a matrix associated with it, that tells Hatch to run the test scripts across different Python versions.\n[[tool.hatch.envs.test.matrix]]\npython = [\"3.10\", \"3.11\", \"3.12\", \"3.13\"]\nIf you run hatch shell test, you will see the output below. To enter an environment with a matrix attached to it, you need to pick the Python environment version that you want to open.\n$ hatch shell test                           \nEnvironment `test` defines a matrix, choose one of the following instead:\n\ntest.py3.10\ntest.py3.11\ntest.py3.12\ntest.py3.13\nOpen the Python 3.13 environment like this:\n$ hatch shell test.py3.13\nTo leave an environment use:\n$ deactivate\n\n\n\nIn the tests section of your pyproject.toml, you will see a tool.hatch.envs.test.scripts table.\nThis table defines the commands that you want Hatch to run in the test environment. Notice that the script has one command called run.\n[tool.hatch.envs.test.scripts]\nrun = \"pytest {args:--cov=greatproject --cov-report=term-missing}\"\nTo run this script , use:\nhatch run test:run\n\nhatch run: calls Hatch and tells it that it will be running a command\ntest:run: defines the environment you want it to run (test) and defines the name of the “script” to berun.\n\nIf you have a Hatch matrix setup for tests, it will both install the necessary Python version using UV and run your tests on each version of the Python versions that you declare in the matrix table. In this case, there are 4 Python versions in the environment, so your tests will run 4 times, once in each Python version listed in the matrix table.\n@lwasser ➜ /workspaces/pyopensci-scipy25-create-python-package (main) $ hatch run test:run\n──────────────────────────────────────────────────────────────────────── test.py3.10 ────────────────────────────────────────────────────────────────────────\n==================================================================== test session starts ====================================================================\nplatform linux -- Python 3.10.16, pytest-8.4.1, pluggy-1.6.0\nUsing --randomly-seed=1490740387\nrootdir: /workspaces/pyopensci-scipy25-create-python-package\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: xdist-3.8.0, randomly-3.16.0, raises-0.11, cov-6.2.1\ncollected 2 items                                                                                                                                           \n\ntests/system/test_import.py .                                                                                                                         [ 50%]\ntests/unit/test_example.py .                                                                                                                          [100%]\n\n====================================================================== tests coverage =======================================================================\n_____________________________________________________ coverage: platform linux, python 3.10.16-final-0 ______________________________________________________\n\nName                           Stmts   Miss Branch BrPart    Cover   Missing\n----------------------------------------------------------------------------\nsrc/greatproject/__init__.py       0      0      0      0  100.00%\nsrc/greatproject/example.py        2      0      0      0  100.00%\n----------------------------------------------------------------------------\nTOTAL                              2      0      0      0  100.00%\n===================================================================== 2 passed in 0.05s =====================================================================\n──────────────────────────────────────────────────────────────────────── test.py3.11 ────────────────────────────────────────────────────────────────────────\n==================================================================== test session starts ====================================================================\nplatform linux -- Python 3.11.12, pytest-8.4.1, pluggy-1.6.0\nUsing --randomly-seed=1596865075\nrootdir: /workspaces/pyopensci-scipy25-create-python-package\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: xdist-3.8.0, randomly-3.16.0, raises-0.11, cov-6.2.1\ncollected 2 items                                                                                                                                           \n\ntests/system/test_import.py .                                                                                                                         [ 50%]\ntests/unit/test_example.py .                                                                                                                          [100%]\n\n====================================================================== tests coverage =======================================================================\n_____________________________________________________ coverage: platform linux, python 3.11.12-final-0 ______________________________________________________\n\nName                           Stmts   Miss Branch BrPart    Cover   Missing\n----------------------------------------------------------------------------\nsrc/greatproject/__init__.py       0      0      0      0  100.00%\nsrc/greatproject/example.py        2      0      0      0  100.00%\n----------------------------------------------------------------------------\nTOTAL                              2      0      0      0  100.00%\n===================================================================== 2 passed in 0.05s =====================================================================\n\n\n\nYou can build your package using the environment and scripts defined in the build tables:\nhatch run build:check\nThis script builds and checks the output distribution files of your package.\nThis build environment table declares that pip and twine should be added to that environment. Adding pip to the environment ensures that it is a current, up-to-date version.\n[tool.hatch.envs.build]\ndescription = \"\"\"Build and test your package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\ndetached = true\n# This table installs created the command hatch run install:check which will build and check your package.\n[tool.hatch.envs.install.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\nThis uses the above environment and tells hatch to run\n\npip check, # verifies your dependencies\nhatch build --clean\n\ntwine check dist/* # this checks your distribution for metadata and other potential issues. to build and test your package."
  },
  {
    "objectID": "DEVELOPMENT.html#build-your-package",
    "href": "DEVELOPMENT.html#build-your-package",
    "title": "Development Guide",
    "section": "",
    "text": "You can build your package using the environment and scripts defined in the build tables:\nhatch run build:check\nThis script builds and checks the output distribution files of your package.\nThis build environment table declares that pip and twine should be added to that environment. Adding pip to the environment ensures that it is a current, up-to-date version.\n[tool.hatch.envs.build]\ndescription = \"\"\"Build and test your package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\ndetached = true\n# This table installs created the command hatch run install:check which will build and check your package.\n[tool.hatch.envs.install.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\nThis uses the above environment and tells hatch to run\n\npip check, # verifies your dependencies\nhatch build --clean\n\ntwine check dist/* # this checks your distribution for metadata and other potential issues. to build and test your package."
  },
  {
    "objectID": "reference/detect_anomalies.html",
    "href": "reference/detect_anomalies.html",
    "title": "detect_anomalies",
    "section": "",
    "text": "detect_anomalies\n\n\n\n\n\nName\nDescription\n\n\n\n\ndetect_anomalies\nThis function flags outliers in numeric columns using either the\n\n\n\n\n\ndetect_anomalies.detect_anomalies(df, method='zscore')\nThis function flags outliers in numeric columns using either the Z-score method or the IQR method.\nOutliers in a dataset can heavily impact our analysis negatively. This function helps to identify potential anomalies in numeric columns of a pandas DataFrame, whether the data is normally distributed or skewed.\nThe function takes in a DataFrame, automatically identifies numeric columns, and applies the specified method to flag outliers. Each numeric column is analyzed independently to detect anomalous values.\nThe z-score method is suitable for normally distributed data but sensitive to extreme outliers, while the IQR method is better for skewed distributions and robust to extreme values.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing numeric columns to analyze for anomalies. Non-numeric columns will be excluded from the analysis.\nrequired\n\n\nmethod\nstr\nThe anomaly detection method to use. Valid options are: - ‘zscore’ : For normally distributed data - ‘iqr’ : For skewed data or robust detection\n'zscore'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple of (pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame containing only the numeric columns plus additional boolean columns (named as ’{column}_outlier’) indicating whether each value is an outlier. True indicates an outlier, False indicates a normal value. outlier_percentage : float The percentage of outliers detected across all numeric columns, calculated as (total outliers / total values) * 100.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf method is not ‘zscore’ or ‘iqr’, or if any numeric column contains fewer than 3 data points, or if any numeric column contains NaN values.\n\n\n\nTypeError\nIf df is not a pandas DataFrame or contains no numeric columns.\n\n\n\n\n\n\nZ-score method: Identifies points that are more than 2 standard deviations away from the mean. The z-score is calculated as: z = (x - mean) / std. A data point is flagged as an outlier if |z| &gt; 2.\nIQR method: Uses the interquartile range to identify outliers. Calculates Q1 (25th percentile) and Q3 (75th percentile), then IQR = Q3 - Q1. A data point is flagged as an outlier if it falls below Q1 - 1.5×IQR or above Q3 + 1.5×IQR.\nAssumptions: - Requires at least 3 data points per numeric column for meaningful analysis - Non-numeric columns are automatically excluded - Missing values (NaN) are not flagged as outliers but will raise an error if present in numeric columns.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Create sample data with clear outliers\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'temperature': [20, 21, 22, 19, 98, 23],\n...     'humidity': [45, 50, 48, 52, 49, 200],\n...     'location': ['A', 'B', 'C', 'D', 'E', 'F']\n... })\n&gt;&gt;&gt; result_df, pct = detect_anomalies(df, method='zscore')\n&gt;&gt;&gt; print(result_df)\n   temperature  humidity  temperature_outlier  humidity_outlier\n0           20        45                False             False\n1           21        50                False             False\n2           22        48                False             False\n3           19        52                False             False\n4           98        49                 True             False\n5           23       200                False              True\n&gt;&gt;&gt; print(f\"{pct:.1f}% of data points are outliers\")\n16.7% of data points are outliers",
    "crumbs": [
      "Reference",
      "Some functions",
      "detect_anomalies"
    ]
  },
  {
    "objectID": "reference/detect_anomalies.html#functions",
    "href": "reference/detect_anomalies.html#functions",
    "title": "detect_anomalies",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndetect_anomalies\nThis function flags outliers in numeric columns using either the\n\n\n\n\n\ndetect_anomalies.detect_anomalies(df, method='zscore')\nThis function flags outliers in numeric columns using either the Z-score method or the IQR method.\nOutliers in a dataset can heavily impact our analysis negatively. This function helps to identify potential anomalies in numeric columns of a pandas DataFrame, whether the data is normally distributed or skewed.\nThe function takes in a DataFrame, automatically identifies numeric columns, and applies the specified method to flag outliers. Each numeric column is analyzed independently to detect anomalous values.\nThe z-score method is suitable for normally distributed data but sensitive to extreme outliers, while the IQR method is better for skewed distributions and robust to extreme values.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing numeric columns to analyze for anomalies. Non-numeric columns will be excluded from the analysis.\nrequired\n\n\nmethod\nstr\nThe anomaly detection method to use. Valid options are: - ‘zscore’ : For normally distributed data - ‘iqr’ : For skewed data or robust detection\n'zscore'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple of (pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame containing only the numeric columns plus additional boolean columns (named as ’{column}_outlier’) indicating whether each value is an outlier. True indicates an outlier, False indicates a normal value. outlier_percentage : float The percentage of outliers detected across all numeric columns, calculated as (total outliers / total values) * 100.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf method is not ‘zscore’ or ‘iqr’, or if any numeric column contains fewer than 3 data points, or if any numeric column contains NaN values.\n\n\n\nTypeError\nIf df is not a pandas DataFrame or contains no numeric columns.\n\n\n\n\n\n\nZ-score method: Identifies points that are more than 2 standard deviations away from the mean. The z-score is calculated as: z = (x - mean) / std. A data point is flagged as an outlier if |z| &gt; 2.\nIQR method: Uses the interquartile range to identify outliers. Calculates Q1 (25th percentile) and Q3 (75th percentile), then IQR = Q3 - Q1. A data point is flagged as an outlier if it falls below Q1 - 1.5×IQR or above Q3 + 1.5×IQR.\nAssumptions: - Requires at least 3 data points per numeric column for meaningful analysis - Non-numeric columns are automatically excluded - Missing values (NaN) are not flagged as outliers but will raise an error if present in numeric columns.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Create sample data with clear outliers\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'temperature': [20, 21, 22, 19, 98, 23],\n...     'humidity': [45, 50, 48, 52, 49, 200],\n...     'location': ['A', 'B', 'C', 'D', 'E', 'F']\n... })\n&gt;&gt;&gt; result_df, pct = detect_anomalies(df, method='zscore')\n&gt;&gt;&gt; print(result_df)\n   temperature  humidity  temperature_outlier  humidity_outlier\n0           20        45                False             False\n1           21        50                False             False\n2           22        48                False             False\n3           19        52                False             False\n4           98        49                 True             False\n5           23       200                False              True\n&gt;&gt;&gt; print(f\"{pct:.1f}% of data points are outliers\")\n16.7% of data points are outliers",
    "crumbs": [
      "Reference",
      "Some functions",
      "detect_anomalies"
    ]
  },
  {
    "objectID": "reference/remove_duplicates.html",
    "href": "reference/remove_duplicates.html",
    "title": "remove_duplicates",
    "section": "",
    "text": "remove_duplicates\n\n\n\n\n\nName\nDescription\n\n\n\n\nremove_duplicates\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary\n\n\n\n\n\nremove_duplicates.remove_duplicates(df, cols=None, keep='first', report=False)\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary report of the duplicate rows, including number of rows after removing duplicates, total number of rows remaining and the keep strategy used.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput DataFrame to check for and remove duplicate rows from.\nrequired\n\n\ncols\n\nOptional parameter. Subset of column names to consider when identifying duplicates. If None, all columns are used to identify duplicate rows.\nNone\n\n\nkeep\n(first, last, False)\nOptional parameter. Determines which duplicate rows to keep: - “first”: keep the first occurrence and remove subsequent duplicates - “last”: keep the last occurrence and remove earlier duplicates - False: remove all duplicate rows\n\"first\"\n\n\nreport\nbool\nOptional parameter. If True, returns a summary report containing information about duplicate rows.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nThe input DataFrame with duplicate rows removed.\n\n\n\n(dict, optional)\nIf report is True, a dictionary summarizing duplicate detection results is returned. The dictionary output includes: - total_rows: int, number of rows in the original input DataFrame - duplicate_rows: int, number of duplicate rows detected - rows_removed: int, number of rows removed - strategy: keep strategy used to remove duplicates, if any - cols_used: list of str or None(i.e. all), columns used for duplicate detection\n\n\n\n\n\n\nTypeError If df is not a pandas DataFrame. KeyError If any column in cols does not exist in the DataFrame. ValueError If keep is not one of {“first”, “last”, False}.\n\n\n\n\nThis function is intended for early-stage data cleaning and EDA processes.\nMissing values are considered in duplicate detection. If two or more rows contain missing values in the same places, they are still considered duplicates.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"id\": [1, 1, 2, 3],\n...     \"value\": [\"A\", \"A\", \"B\", \"C\"]\n... })\nRemove duplicate rows using all columns:\n&gt;&gt;&gt; cleaned_df = remove_duplicates(df)\n&gt;&gt;&gt; cleaned_df\n   id value\n0   1     A\n2   2     B\n3   3     C\nRemove duplicates based on some specified columns and return a summary report:\n&gt;&gt;&gt; cleaned_df, report = remove_duplicates(df,cols=[\"id\"],keep=\"last\",report=True)\n&gt;&gt;&gt; cleaned_df\n   id value\n1   1     A\n2   2     B\n3   3     C\n&gt;&gt;&gt; report\n{'total_rows': 4,\n'duplicate_rows': 1,\n'rows_removed': 1,\n'cols_used': ['id']}",
    "crumbs": [
      "Reference",
      "Some functions",
      "remove_duplicates"
    ]
  },
  {
    "objectID": "reference/remove_duplicates.html#functions",
    "href": "reference/remove_duplicates.html#functions",
    "title": "remove_duplicates",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nremove_duplicates\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary\n\n\n\n\n\nremove_duplicates.remove_duplicates(df, cols=None, keep='first', report=False)\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary report of the duplicate rows, including number of rows after removing duplicates, total number of rows remaining and the keep strategy used.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput DataFrame to check for and remove duplicate rows from.\nrequired\n\n\ncols\n\nOptional parameter. Subset of column names to consider when identifying duplicates. If None, all columns are used to identify duplicate rows.\nNone\n\n\nkeep\n(first, last, False)\nOptional parameter. Determines which duplicate rows to keep: - “first”: keep the first occurrence and remove subsequent duplicates - “last”: keep the last occurrence and remove earlier duplicates - False: remove all duplicate rows\n\"first\"\n\n\nreport\nbool\nOptional parameter. If True, returns a summary report containing information about duplicate rows.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nThe input DataFrame with duplicate rows removed.\n\n\n\n(dict, optional)\nIf report is True, a dictionary summarizing duplicate detection results is returned. The dictionary output includes: - total_rows: int, number of rows in the original input DataFrame - duplicate_rows: int, number of duplicate rows detected - rows_removed: int, number of rows removed - strategy: keep strategy used to remove duplicates, if any - cols_used: list of str or None(i.e. all), columns used for duplicate detection\n\n\n\n\n\n\nTypeError If df is not a pandas DataFrame. KeyError If any column in cols does not exist in the DataFrame. ValueError If keep is not one of {“first”, “last”, False}.\n\n\n\n\nThis function is intended for early-stage data cleaning and EDA processes.\nMissing values are considered in duplicate detection. If two or more rows contain missing values in the same places, they are still considered duplicates.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"id\": [1, 1, 2, 3],\n...     \"value\": [\"A\", \"A\", \"B\", \"C\"]\n... })\nRemove duplicate rows using all columns:\n&gt;&gt;&gt; cleaned_df = remove_duplicates(df)\n&gt;&gt;&gt; cleaned_df\n   id value\n0   1     A\n2   2     B\n3   3     C\nRemove duplicates based on some specified columns and return a summary report:\n&gt;&gt;&gt; cleaned_df, report = remove_duplicates(df,cols=[\"id\"],keep=\"last\",report=True)\n&gt;&gt;&gt; cleaned_df\n   id value\n1   1     A\n2   2     B\n3   3     C\n&gt;&gt;&gt; report\n{'total_rows': 4,\n'duplicate_rows': 1,\n'rows_removed': 1,\n'cols_used': ['id']}",
    "crumbs": [
      "Reference",
      "Some functions",
      "remove_duplicates"
    ]
  }
]