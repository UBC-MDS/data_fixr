[
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct aims to create a welcoming, inclusive and harassment-free environment for all contributors. All participants are expected to treat each other with respect, and abusive or inappropriate behavior will not be tolerated.\n\n\nAs contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, submitting pull requests and other activities.\nWe are committed to making participation in this project a harassment-free experience for everybody, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion or nationality.\n\n\n\nTo foster an open and welcoming community, all contributors are expected to:\n\nUse welcoming and inclusive language in all interactions\nBe considerate of differing viewpoints and experiences\nAccept constructive criticism\nFocus on what is best for the community\nWork constructively with other contributors and be open to feedback\n\n\n\n\nExamples of unacceptable behavior by contributors include:\n\nThe use of sexualized language or imagery\nPersonal attacks, trolling or insulting/derogatory comments\nPublic or private harassment\nPublishing others’ private information, such as physical or electronic addresses, without explicit permission\nOther conduct which could reasonably be considered inappropriate, unethical or unprofessional\n\n\n\nProject maintainers have the right and responsibility to review and, if necessary, remove, edit or reject comments, code, issues and other contributions that are not aligned with this Code of Conduct.\nConsequences of violations may include:\n\nA written warning regarding their behavior\nTemporary restriction from interacting with the project for a specified period\nPermanent removal from the project and all associated spaces\n\nThe severity of the consequences will depend on the nature and frequency of the violation.\n\n\n\nInstances of abusive, harassing or otherwise unacceptable behavior may be reported by contacting any member of the project team directly or by e-mail: asrivas1@student.ubc.ca.\nWhen reporting an incident, please include your name and contact information, a description of the incident and any additional context that might be helpful. All complaints will be reviewed and investigated promptly and will result in a response that is deemed appropriate to the circumstances. All reports will be kept strictly confidential.\n\n\n\n\nThis Code of Conduct is adapted from the Pandas Code of Conduct, which was adapted from the Contributor Covenant, version 1.3.0, available at https://www.contributor-covenant.org/version/1/3/0/code-of-conduct/ and the Swift Code of Conduct."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#statement-on-diversity-and-inclusion",
    "href": "CODE_OF_CONDUCT.html#statement-on-diversity-and-inclusion",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "As contributors and maintainers of this project, and in the interest of fostering an open and welcoming community, we pledge to respect all people who contribute through reporting issues, posting feature requests, submitting pull requests and other activities.\nWe are committed to making participation in this project a harassment-free experience for everybody, regardless of level of experience, gender, gender identity and expression, sexual orientation, disability, personal appearance, body size, race, ethnicity, age, religion or nationality."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#expected-behavior",
    "href": "CODE_OF_CONDUCT.html#expected-behavior",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "To foster an open and welcoming community, all contributors are expected to:\n\nUse welcoming and inclusive language in all interactions\nBe considerate of differing viewpoints and experiences\nAccept constructive criticism\nFocus on what is best for the community\nWork constructively with other contributors and be open to feedback"
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "href": "CODE_OF_CONDUCT.html#unacceptable-behavior",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Examples of unacceptable behavior by contributors include:\n\nThe use of sexualized language or imagery\nPersonal attacks, trolling or insulting/derogatory comments\nPublic or private harassment\nPublishing others’ private information, such as physical or electronic addresses, without explicit permission\nOther conduct which could reasonably be considered inappropriate, unethical or unprofessional\n\n\n\nProject maintainers have the right and responsibility to review and, if necessary, remove, edit or reject comments, code, issues and other contributions that are not aligned with this Code of Conduct.\nConsequences of violations may include:\n\nA written warning regarding their behavior\nTemporary restriction from interacting with the project for a specified period\nPermanent removal from the project and all associated spaces\n\nThe severity of the consequences will depend on the nature and frequency of the violation.\n\n\n\nInstances of abusive, harassing or otherwise unacceptable behavior may be reported by contacting any member of the project team directly or by e-mail: asrivas1@student.ubc.ca.\nWhen reporting an incident, please include your name and contact information, a description of the incident and any additional context that might be helpful. All complaints will be reviewed and investigated promptly and will result in a response that is deemed appropriate to the circumstances. All reports will be kept strictly confidential."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Pandas Code of Conduct, which was adapted from the Contributor Covenant, version 1.3.0, available at https://www.contributor-covenant.org/version/1/3/0/code-of-conduct/ and the Swift Code of Conduct."
  },
  {
    "objectID": "retrospective.html",
    "href": "retrospective.html",
    "title": "Retrospective",
    "section": "",
    "text": "Here we present a retrospective analysis of the data_fixr project, developed as part of 2025–26 DSCI-524 Group 3.\nThe goal of this retrospective is to reflect on our planning accuracy, workflow organization, tooling decisions, and collaboration practices, using evidence collected from GitHub Projects across all four milestones.\n\nEvidence Used\nThis retrospective is grounded in data from the following GitHub Project views and insights:\n\nMilestone Progress (Table view grouped by milestone)\nBurn-up / Completion Chart (Insights view grouped by milestone and status)\nTeam Workload(Table view grouped by assignee, filtered to completed tasks)\n\nThese views were used to assess scope evolution, bottlenecks, and contribution balance.\n\n\n\nMilestone Progress and Planning Accuracy\nUsing the Milestone Progress view, we observed that workload was the most during Milestone 2.\n\nMilestone 1 primarily focused on project setup, writing function documentations, and scaffolding. (Count of Items: 13)\nMilestone 2 introduced function implementations and unit testing, with a moderate increase in issue count. (Count of Items: 17)\nMilestone 3 showed a moderate amount of issues related to CI/CD configuration, documentation builds, and deployment previews. (Count of Items: 13)\nMilestone 4 had fewer issues overall, but required higher coordination and review effort.\n\nReflection:\nInfrastructure-related tasks, particularly CI/CD and documentation deployment, were underestimated during early planning. While core function development proceeded smoothly, automation and deployment required more iterative debugging than anticipated.\n\n\n\nWorkflow and Bottlenecks\nThe Burn-up / Completion Chart revealed temporary bottlenecks during Milestone 3.\n\nSeveral issues accumulated in the In Progress and Review states while CI/CD workflows were being debugged.\n\nReflection:\nCI/CD setup became the primary bottleneck, delaying dependent tasks. Earlier experimentation with deployment workflows could have reduced friction later in the project. Distribution of Tasks was even in Milestone 3 aswell but that was just the nature of the tasks.\n\n\n\nTeam Contributions and Bus Factor\nThe Team Workload view showed a generally balanced distribution of completed issues across team members.\n\nSome contributors had fewer issues assigned but worked on higher-complexity tasks, such as CI/CD workflows and deployment previews.\nMilestone 3 caused disparity in the distribution of tasks which showed some members to have lower amount of tasks than others.\nOther contributors handled multiple smaller issues related to documentation and testing.\n\nReflection:\nWhile issue counts were not perfectly uniform, the overall workload was balanced when task complexity was considered.\n\n\n\nRetrospective Summary (DAKI)\n\n\nDrop\n\nUsing Slack for communication issues rather than Github issues.\n\n\n\nAdd\n\nEarlier CI/CD prototyping during the project timeline.\nPosting documentation preview links directly in pull request comments.\nClearer pull request templates to standardize reviews.\n\n\n\nKeep\n\nIssue-based task ownership with clear assignees.\nWriting unit tests alongside function implementations.\nUsing GitHub Projects to track progress across milestones.\n\n\n\nImprove\n\nMilestone scoping to better account for infrastructure and automation complexity.\nCross-training team members on CI/CD workflows to reduce bus-factor risk.\n\n\n\n\nTools, Infrastructure, and Scaling Considerations\nThroughout the project, we applied several development tools and practices introduced in this course, including GitHub Actions for testing and deployment, GitHub Projects for task tracking, and structured branching workflows.\nIf this project were to be scaled up or extended (e.g., as a capstone project), we would: - Introduce additional CI checks such as linting and formatting enforcement. - Apply stricter branch protection rules earlier. - Expand documentation automation and preview tooling. - Distribute infrastructure knowledge more evenly across the team.\nThese improvements would enhance maintainability, reliability, and collaboration efficiency as project complexity grows."
  },
  {
    "objectID": "reference/missing_values.html",
    "href": "reference/missing_values.html",
    "title": "missing_values",
    "section": "",
    "text": "missing_values\n\n\n\n\n\nName\nDescription\n\n\n\n\nmissing_values\nThis function fills missing values (NaN) in a pandas DataFrame using\n\n\n\n\n\nmissing_values.missing_values(df, method='median')\nThis function fills missing values (NaN) in a pandas DataFrame using column-appropriate imputation strategies.\nThis function imputes missing values in both numeric and categorical columns. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation.\nMissing values can distort statistical analyses and machine learning models. This function provides common strategies for imputing missing values depending on the nature of the data distribution.\nThe function identifies numeric and non-numeric columns and applies imputation independently to each column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing missing values to be imputed.\nrequired\n\n\nmethod\nstr\nThe imputation method to use for numeric columns. Valid options are: - ‘mean’ : Replace NaN with column mean (suitable for symmetric data) - ‘median’ : Replace NaN with column median (robust to outliers) - ‘mode’ : Replace NaN with column mode Categorical (non-numeric) columns always use mode imputation regardless of the selected method.\n\"median\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n(pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame with missing values filled in both numeric and categorical (non-numeric) columns. filled_percentage : float The percentage of total DataFrame values that were originally missing and have been filled, calculated as: (number of filled values / number of total values) * 100. Columns containing only NaN values are left unchanged and do not contribute any filled values to this percentage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the 3 supported numeric options.\n\n\n\n\n\n\n\nNumeric columns are imputed using the specified method.\nCategorical (non-numeric) columns are imputed using mode.\nImputation is applied column-wise.\nColumns containing all NaN values are left unchanged and do not affect the filled percentage.\nIf multiple modes exist (for both numeric and categorical columns), the first mode returned by pandas is used.\nThe original DataFrame is not modified; a copy is returned.\nThe filled percentage includes values filled in both numeric and categorical (non-numeric) columns.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'age': [25, 30, np.nan, 28],\n...     'income': [50000, np.nan, 52000, np.nan],\n...     'city': ['A', 'B', np.nan, 'B']\n... })\n&gt;&gt;&gt; result_df, filled_percentage = missing_values(df, method='median')\n&gt;&gt;&gt; print(result_df)\n    age   income city\n0  25.0  50000.0    A\n1  30.0  51000.0    B\n2  28.0  52000.0    B\n3  28.0  51000.0    B\n&gt;&gt;&gt; print(f\"{filled_percentage:.1f}% of values were filled.\")\n33.3% of values were filled.",
    "crumbs": [
      "Reference",
      "Some functions",
      "missing_values"
    ]
  },
  {
    "objectID": "reference/missing_values.html#functions",
    "href": "reference/missing_values.html#functions",
    "title": "missing_values",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nmissing_values\nThis function fills missing values (NaN) in a pandas DataFrame using\n\n\n\n\n\nmissing_values.missing_values(df, method='median')\nThis function fills missing values (NaN) in a pandas DataFrame using column-appropriate imputation strategies.\nThis function imputes missing values in both numeric and categorical columns. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation.\nMissing values can distort statistical analyses and machine learning models. This function provides common strategies for imputing missing values depending on the nature of the data distribution.\nThe function identifies numeric and non-numeric columns and applies imputation independently to each column.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing missing values to be imputed.\nrequired\n\n\nmethod\nstr\nThe imputation method to use for numeric columns. Valid options are: - ‘mean’ : Replace NaN with column mean (suitable for symmetric data) - ‘median’ : Replace NaN with column median (robust to outliers) - ‘mode’ : Replace NaN with column mode Categorical (non-numeric) columns always use mode imputation regardless of the selected method.\n\"median\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\n(pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame with missing values filled in both numeric and categorical (non-numeric) columns. filled_percentage : float The percentage of total DataFrame values that were originally missing and have been filled, calculated as: (number of filled values / number of total values) * 100. Columns containing only NaN values are left unchanged and do not contribute any filled values to this percentage.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the 3 supported numeric options.\n\n\n\n\n\n\n\nNumeric columns are imputed using the specified method.\nCategorical (non-numeric) columns are imputed using mode.\nImputation is applied column-wise.\nColumns containing all NaN values are left unchanged and do not affect the filled percentage.\nIf multiple modes exist (for both numeric and categorical columns), the first mode returned by pandas is used.\nThe original DataFrame is not modified; a copy is returned.\nThe filled percentage includes values filled in both numeric and categorical (non-numeric) columns.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'age': [25, 30, np.nan, 28],\n...     'income': [50000, np.nan, 52000, np.nan],\n...     'city': ['A', 'B', np.nan, 'B']\n... })\n&gt;&gt;&gt; result_df, filled_percentage = missing_values(df, method='median')\n&gt;&gt;&gt; print(result_df)\n    age   income city\n0  25.0  50000.0    A\n1  30.0  51000.0    B\n2  28.0  52000.0    B\n3  28.0  51000.0    B\n&gt;&gt;&gt; print(f\"{filled_percentage:.1f}% of values were filled.\")\n33.3% of values were filled.",
    "crumbs": [
      "Reference",
      "Some functions",
      "missing_values"
    ]
  },
  {
    "objectID": "reference/remove_duplicates.html",
    "href": "reference/remove_duplicates.html",
    "title": "remove_duplicates",
    "section": "",
    "text": "remove_duplicates\n\n\n\n\n\nName\nDescription\n\n\n\n\nremove_duplicates\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary\n\n\n\n\n\nremove_duplicates.remove_duplicates(df, cols=None, keep='first', report=False)\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary report of the duplicate rows, including number of rows after removing duplicates, total number of rows remaining and the keep strategy used.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput DataFrame to check for and remove duplicate rows from.\nrequired\n\n\ncols\n\nOptional parameter. Subset of column names to consider when identifying duplicates. If None, all columns are used to identify duplicate rows.\nNone\n\n\nkeep\n(first, last, False)\nOptional parameter. Determines which duplicate rows to keep: - “first”: keep the first occurrence and remove subsequent duplicates - “last”: keep the last occurrence and remove earlier duplicates - False: remove all duplicate rows\n\"first\"\n\n\nreport\nbool\nOptional parameter. If True, returns a summary report containing information about duplicate rows.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nThe input DataFrame with duplicate rows removed.\n\n\n\n(dict, optional)\nIf report is True, a dictionary summarizing duplicate detection results is returned. The dictionary output includes: - total_rows: int, number of rows in the original input DataFrame - duplicate_rows: int, number of duplicate rows detected - rows_removed: int, number of rows removed - strategy: keep strategy used to remove duplicates, if any - cols_used: list of str or None(i.e. all), columns used for duplicate detection\n\n\n\n\n\n\nTypeError If df is not a pandas DataFrame. KeyError If any column in cols does not exist in the DataFrame. ValueError If keep is not one of {“first”, “last”, False}.\n\n\n\n\nThis function is intended for early-stage data cleaning and EDA processes.\nMissing values are considered in duplicate detection. If two or more rows contain missing values in the same places, they are still considered duplicates.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"id\": [1, 1, 2, 3],\n...     \"value\": [\"A\", \"A\", \"B\", \"C\"]\n... })\nRemove duplicate rows using all columns:\n&gt;&gt;&gt; cleaned_df = remove_duplicates(df)\n&gt;&gt;&gt; cleaned_df\n   id value\n0   1     A\n2   2     B\n3   3     C\nRemove duplicates based on some specified columns and return a summary report:\n&gt;&gt;&gt; cleaned_df, report = remove_duplicates(df,cols=[\"id\"],keep=\"last\",report=True)\n&gt;&gt;&gt; cleaned_df\n   id value\n1   1     A\n2   2     B\n3   3     C\n&gt;&gt;&gt; report\n{'total_rows': 4,\n'duplicate_rows': 1,\n'rows_removed': 1,\n'cols_used': ['id']}",
    "crumbs": [
      "Reference",
      "Some functions",
      "remove_duplicates"
    ]
  },
  {
    "objectID": "reference/remove_duplicates.html#functions",
    "href": "reference/remove_duplicates.html#functions",
    "title": "remove_duplicates",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\nremove_duplicates\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary\n\n\n\n\n\nremove_duplicates.remove_duplicates(df, cols=None, keep='first', report=False)\nIdentifies and removes duplicate rows for a given dataframe. Optionally, returns a summary report of the duplicate rows, including number of rows after removing duplicates, total number of rows remaining and the keep strategy used.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput DataFrame to check for and remove duplicate rows from.\nrequired\n\n\ncols\n\nOptional parameter. Subset of column names to consider when identifying duplicates. If None, all columns are used to identify duplicate rows.\nNone\n\n\nkeep\n(first, last, False)\nOptional parameter. Determines which duplicate rows to keep: - “first”: keep the first occurrence and remove subsequent duplicates - “last”: keep the last occurrence and remove earlier duplicates - False: remove all duplicate rows\n\"first\"\n\n\nreport\nbool\nOptional parameter. If True, returns a summary report containing information about duplicate rows.\nFalse\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nThe input DataFrame with duplicate rows removed.\n\n\n\n(dict, optional)\nIf report is True, a dictionary summarizing duplicate detection results is returned. The dictionary output includes: - total_rows: int, number of rows in the original input DataFrame - duplicate_rows: int, number of duplicate rows detected - rows_removed: int, number of rows removed - strategy: keep strategy used to remove duplicates, if any - cols_used: list of str or None(i.e. all), columns used for duplicate detection\n\n\n\n\n\n\nTypeError If df is not a pandas DataFrame. KeyError If any column in cols does not exist in the DataFrame. ValueError If keep is not one of {“first”, “last”, False}.\n\n\n\n\nThis function is intended for early-stage data cleaning and EDA processes.\nMissing values are considered in duplicate detection. If two or more rows contain missing values in the same places, they are still considered duplicates.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"id\": [1, 1, 2, 3],\n...     \"value\": [\"A\", \"A\", \"B\", \"C\"]\n... })\nRemove duplicate rows using all columns:\n&gt;&gt;&gt; cleaned_df = remove_duplicates(df)\n&gt;&gt;&gt; cleaned_df\n   id value\n0   1     A\n2   2     B\n3   3     C\nRemove duplicates based on some specified columns and return a summary report:\n&gt;&gt;&gt; cleaned_df, report = remove_duplicates(df,cols=[\"id\"],keep=\"last\",report=True)\n&gt;&gt;&gt; cleaned_df\n   id value\n1   1     A\n2   2     B\n3   3     C\n&gt;&gt;&gt; report\n{'total_rows': 4,\n'duplicate_rows': 1,\n'rows_removed': 1,\n'cols_used': ['id']}",
    "crumbs": [
      "Reference",
      "Some functions",
      "remove_duplicates"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Code Coverage\n\n\n\nBuild\n\n\n\nDocumentation\n\n\n\nDeploy\n\n\n\nPython\n\n\n\n\n\n\ndata_fixr is a lightweight Python package designed to support exploratory data analysis and early-stage data cleaning for tabular datasets. The package provides standardized, machine-readable diagnostics and cleaning utilities that help users quickly assess data quality, identify common issues, and prepare datasets for downstream analysis or machine learning workflows. Rather than generating plots or reports, data_fixr focuses on returning clean, structured outputs that can be easily tested, logged, or integrated into automated pipelines.\n\n\n\nTo view usage examples and function details, refer to the reference page of the documentation.\n\n\nA common early-stage data analysis workflow involves checking for correlations, missing values, duplicates, and anomalies before modeling. data_fixr allows these checks to be performed with minimal code and consistent outputs:\nfrom data_fixr import (\n    correlation_report,\n    remove_duplicates,\n    detect_anomalies,\n    missing_values\n)\nimport pandas as pd\n# Note: Above code can also be used to verify installation via python interpreter\n\ndata = pd.DataFrame({\n    'age': [25, 30, 25, 150, 35, 30, 28, 45, 32, 29],  # 150 is an anomaly\n    'income': [50000, 60000, 50000, 70000, None, 60000, 55000, 85000, 62000, 58000], #contains missing value and duplicates\n    'years_experience': [3, 7, 3, 25, 10, 7, 5, 20, 8, 6]\n})\n\n# Fill missing values and fill them with method 'mode'\nmissing_report = missing_values(data, method='mode')\n\n# Identify and remove duplicate rows, keeping the first instance of the duplicated observation\nclean_data = remove_duplicates(data, keep='first', report=True)\n\n# Find outliers or anomalous values with default zscore method in the age and years experience columns\nanomalies = detect_anomalies(data[['age', 'years_experience']])\n\n# Generate a correlation report showing pairwise correlations between all numeric variables in the data using the spearman correlation method:\ncorr_report = correlation_report(data, method='spearman')\nNote: To view an end-to-end example workflow, refer to the tutorial page of the documentation.\n\n\n\n\n\n\ngit clone https://github.com/UBC-MDS/data_fixr.git\ncd data_fixr\npip install -e .\n\n\n\nTo install the latest development release of data_fixr from Test PyPI:\npip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ data-fixr\nIf you already have data_fixr installed and want to upgrade to the newest available version on Test PyPI, run:\npip install --upgrade -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ data-fixr\n\n\n\n\n\ncorrelation_report(): Computes pairwise correlations between numeric columns in a pandas DataFrame and returns a long-format diagnostic table. The output summarizes the strength and direction of relationships between features in a standardized, machine-readable format suitable for exploratory analysis and preprocessing workflows.\nremove_duplicates(): Identifies and removes duplicate rows for a given dataset. Users can specify the retention strategy for handling duplicates and choose whether duplicates are detected using all columns or a specified subset. Optionally, the function can return a useful summary report describing the number of duplicate rows detected and removed.\ndetect_anomalies()’: This function identifies and flags outliers in numeric columns of a pandas DataFrame using either the Z-score method (for normally distributed data) or the IQR (Interquartile Range) method (for skewed data). It returns a DataFrame with the numeric columns and boolean outlier flags for each numeric column, as well as the overall percentage of outliers detected.\nmissing_values: This function fills missing values (NaN) in both numeric and categorical (non-numeric) columns of a pandas DataFrame. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation. The function returns a DataFrame with missing values filled and the overall percentage of missing values that were imputed across all columns.\n\n\n\n\nTo contribute, run the tests, or build documentation, follow the steps below.\nFirst, clone the repository and navigate to the root:\n$ git clone https://github.com/UBC-MDS/data_fixr.git\n$ cd data_fixr\n(Optional) To run the package in a clean environment with Python 3.11.\n$ conda env create -f environment.yml\n$ conda activate data_fixr\nInstall the package in editable mode:\n$ pip install -e .\nInstall the required dependencies:\n$ pip install -e \".[tests, dev, docs]\"\nTo run tests:\n$ pytest --cov=src --cov-branch --cov-report=term-missing\nTo build documentation locally:\nquartodoc build\nquarto render \nTo preview the documentation:\nquarto preview\nDocumentation is automatically built and deployed via GitHub Actions when changes are merged into main.\nIf you have opted to use the conda environment, deactivate the environment once finished with:\n$ conda deactivate\n\n\n\nThe functionality provided by data_fixr overlaps partially with capabilities available in established Python data analysis libraries, most notably pandas and scikit-learn. Pandas provides low-level methods for computing correlations between numeric variables and for removing duplicate rows, while scikit-learn and related libraries offer utilities for identifying outliers as part of preprocessing or feature selection workflows. However, these tools typically expose such functionality as individual operations without producing standardized, diagnostics-oriented summaries.\nIn particular, while duplicate removal and missing-value imputation functionality already exist in pandas, the corresponding functions in data_fixr extend this behavior by optionally returning structured summary information. The duplicate removal function can return a summary report describing how many duplicate rows were detected and removed, while the missing-value filling function reports the overall percentage of missing values that were imputed across the dataset. Additionally, automated exploratory data analysis tools such as ydata-profiling focus on generating comprehensive visual or HTML reports, whereas data_fixr emphasizes lightweight, modular functions that return machine-readable outputs intended for exploratory diagnostics, reproducible preprocessing pipelines, and downstream machine learning workflows.\n\n\n\n\nPython ≥ 3.10\n\n\n\n\n\nApoorva Srivastava\nChikire Aku-Ibe\nNour Shawky\nZain Nofal\n\n\n\n\n\nCopyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe.\nFree software distributed under the MIT License."
  },
  {
    "objectID": "index.html#package-summary",
    "href": "index.html#package-summary",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "data_fixr is a lightweight Python package designed to support exploratory data analysis and early-stage data cleaning for tabular datasets. The package provides standardized, machine-readable diagnostics and cleaning utilities that help users quickly assess data quality, identify common issues, and prepare datasets for downstream analysis or machine learning workflows. Rather than generating plots or reports, data_fixr focuses on returning clean, structured outputs that can be easily tested, logged, or integrated into automated pipelines."
  },
  {
    "objectID": "index.html#documentation",
    "href": "index.html#documentation",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "To view usage examples and function details, refer to the reference page of the documentation.\n\n\nA common early-stage data analysis workflow involves checking for correlations, missing values, duplicates, and anomalies before modeling. data_fixr allows these checks to be performed with minimal code and consistent outputs:\nfrom data_fixr import (\n    correlation_report,\n    remove_duplicates,\n    detect_anomalies,\n    missing_values\n)\nimport pandas as pd\n# Note: Above code can also be used to verify installation via python interpreter\n\ndata = pd.DataFrame({\n    'age': [25, 30, 25, 150, 35, 30, 28, 45, 32, 29],  # 150 is an anomaly\n    'income': [50000, 60000, 50000, 70000, None, 60000, 55000, 85000, 62000, 58000], #contains missing value and duplicates\n    'years_experience': [3, 7, 3, 25, 10, 7, 5, 20, 8, 6]\n})\n\n# Fill missing values and fill them with method 'mode'\nmissing_report = missing_values(data, method='mode')\n\n# Identify and remove duplicate rows, keeping the first instance of the duplicated observation\nclean_data = remove_duplicates(data, keep='first', report=True)\n\n# Find outliers or anomalous values with default zscore method in the age and years experience columns\nanomalies = detect_anomalies(data[['age', 'years_experience']])\n\n# Generate a correlation report showing pairwise correlations between all numeric variables in the data using the spearman correlation method:\ncorr_report = correlation_report(data, method='spearman')\nNote: To view an end-to-end example workflow, refer to the tutorial page of the documentation."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "git clone https://github.com/UBC-MDS/data_fixr.git\ncd data_fixr\npip install -e .\n\n\n\nTo install the latest development release of data_fixr from Test PyPI:\npip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ data-fixr\nIf you already have data_fixr installed and want to upgrade to the newest available version on Test PyPI, run:\npip install --upgrade -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ data-fixr"
  },
  {
    "objectID": "index.html#functions",
    "href": "index.html#functions",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "correlation_report(): Computes pairwise correlations between numeric columns in a pandas DataFrame and returns a long-format diagnostic table. The output summarizes the strength and direction of relationships between features in a standardized, machine-readable format suitable for exploratory analysis and preprocessing workflows.\nremove_duplicates(): Identifies and removes duplicate rows for a given dataset. Users can specify the retention strategy for handling duplicates and choose whether duplicates are detected using all columns or a specified subset. Optionally, the function can return a useful summary report describing the number of duplicate rows detected and removed.\ndetect_anomalies()’: This function identifies and flags outliers in numeric columns of a pandas DataFrame using either the Z-score method (for normally distributed data) or the IQR (Interquartile Range) method (for skewed data). It returns a DataFrame with the numeric columns and boolean outlier flags for each numeric column, as well as the overall percentage of outliers detected.\nmissing_values: This function fills missing values (NaN) in both numeric and categorical (non-numeric) columns of a pandas DataFrame. Numeric columns are filled using a user-specified method (mean, median, or mode), while categorical (non-numeric) columns are automatically filled using mode imputation. The function returns a DataFrame with missing values filled and the overall percentage of missing values that were imputed across all columns."
  },
  {
    "objectID": "index.html#development-and-testing",
    "href": "index.html#development-and-testing",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "To contribute, run the tests, or build documentation, follow the steps below.\nFirst, clone the repository and navigate to the root:\n$ git clone https://github.com/UBC-MDS/data_fixr.git\n$ cd data_fixr\n(Optional) To run the package in a clean environment with Python 3.11.\n$ conda env create -f environment.yml\n$ conda activate data_fixr\nInstall the package in editable mode:\n$ pip install -e .\nInstall the required dependencies:\n$ pip install -e \".[tests, dev, docs]\"\nTo run tests:\n$ pytest --cov=src --cov-branch --cov-report=term-missing\nTo build documentation locally:\nquartodoc build\nquarto render \nTo preview the documentation:\nquarto preview\nDocumentation is automatically built and deployed via GitHub Actions when changes are merged into main.\nIf you have opted to use the conda environment, deactivate the environment once finished with:\n$ conda deactivate"
  },
  {
    "objectID": "index.html#position-in-python-environment",
    "href": "index.html#position-in-python-environment",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "The functionality provided by data_fixr overlaps partially with capabilities available in established Python data analysis libraries, most notably pandas and scikit-learn. Pandas provides low-level methods for computing correlations between numeric variables and for removing duplicate rows, while scikit-learn and related libraries offer utilities for identifying outliers as part of preprocessing or feature selection workflows. However, these tools typically expose such functionality as individual operations without producing standardized, diagnostics-oriented summaries.\nIn particular, while duplicate removal and missing-value imputation functionality already exist in pandas, the corresponding functions in data_fixr extend this behavior by optionally returning structured summary information. The duplicate removal function can return a summary report describing how many duplicate rows were detected and removed, while the missing-value filling function reports the overall percentage of missing values that were imputed across the dataset. Additionally, automated exploratory data analysis tools such as ydata-profiling focus on generating comprehensive visual or HTML reports, whereas data_fixr emphasizes lightweight, modular functions that return machine-readable outputs intended for exploratory diagnostics, reproducible preprocessing pipelines, and downstream machine learning workflows."
  },
  {
    "objectID": "index.html#requirements",
    "href": "index.html#requirements",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Python ≥ 3.10"
  },
  {
    "objectID": "index.html#contributors",
    "href": "index.html#contributors",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Apoorva Srivastava\nChikire Aku-Ibe\nNour Shawky\nZain Nofal"
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Welcome to data_fixr",
    "section": "",
    "text": "Copyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe.\nFree software distributed under the MIT License."
  },
  {
    "objectID": "DEVELOPMENT.html",
    "href": "DEVELOPMENT.html",
    "title": "Development Guide",
    "section": "",
    "text": "Welcome to your shiny new package. This page will help you get started with using Hatch to manage your package.\nIf you look at your project, you will see that a pyproject.toml file. This file stores both your package configuration and settings for development tools like Hatch that you will use to work on your package.\nThis file is written using a .toml format. You can learn more about toml here. Here’s the TL&DR:\n\nEach [] section in the toml file is called a table.\nYou can nest tables with double brackets like this[[]]\nTables contain information about a element that you want to configure.\n\nWe are using Hatch as the default packaging tool. Hatch allows you to configure and run environments and scripts similar to workflow tools like tox or nox.\nHach, by default, uses virtual environments (venv) to manage environments. But you can configure it to use other environment tools.Read the hatch documentation to learn more about environments.\nFor this template, we have set up Hatch environments for you to use. At the bottom of your pyproject.toml file, notice a hatch environment section that looks like this:\n########################################\n# Hatch Environments\n########################################\nBelow is the Hatch environment to install your package. Notice that it defines pip and twine as two packages that the environment needs.\n[tool.hatch.envs.build]\ndescription = \"\"\"Test the installation the package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\nThe table below defines the scripts that you will run build and check your package.\n[tool.hatch.envs.build.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\ndetached = true\nYou can enter that environment to check it out:\n$ hatch shell build\nIf you run pip list, in the environment, twine will be there:\n$ pip list\nHatch by default, installs your package in editable mode (-e) into its virtual environments. But if detached=True is set, then it will skip installing your package into the virtual enviornment.\n\n\nBelow you see the Hatch environment test table.\ntool.hatch.envs says, “Hey, Hatch, this is the definition for an environment.” test is the name of the environment.\nThe environment below defines the dependencies that Hatch needs to install into the environment named test.\n[tool.hatch.envs.test]\ndescription = \"\"\"Run the test suite.\"\"\"\ndependencies = [\n    \"pytest\",\n    \"pytest-cov\",\n    \"pytest-raises\",\n    \"pytest-randomly\",\n    \"pytest-xdist\",\n]\nTo enter a Hatch environment use:\nhatch shell environmentname\nSo you can enter the test environment above with:\nhatch shell test\n\n\n\nIf the environment has a matrix associated with it, that tells Hatch to run the test scripts across different Python versions.\n[[tool.hatch.envs.test.matrix]]\npython = [\"3.10\", \"3.11\", \"3.12\", \"3.13\"]\nIf you run hatch shell test, you will see the output below. To enter an environment with a matrix attached to it, you need to pick the Python environment version that you want to open.\n$ hatch shell test                           \nEnvironment `test` defines a matrix, choose one of the following instead:\n\ntest.py3.10\ntest.py3.11\ntest.py3.12\ntest.py3.13\nOpen the Python 3.13 environment like this:\n$ hatch shell test.py3.13\nTo leave an environment use:\n$ deactivate\n\n\n\nIn the tests section of your pyproject.toml, you will see a tool.hatch.envs.test.scripts table.\nThis table defines the commands that you want Hatch to run in the test environment. Notice that the script has one command called run.\n[tool.hatch.envs.test.scripts]\nrun = \"pytest {args:--cov=greatproject --cov-report=term-missing}\"\nTo run this script , use:\nhatch run test:run\n\nhatch run: calls Hatch and tells it that it will be running a command\ntest:run: defines the environment you want it to run (test) and defines the name of the “script” to berun.\n\nIf you have a Hatch matrix setup for tests, it will both install the necessary Python version using UV and run your tests on each version of the Python versions that you declare in the matrix table. In this case, there are 4 Python versions in the environment, so your tests will run 4 times, once in each Python version listed in the matrix table.\n@lwasser ➜ /workspaces/pyopensci-scipy25-create-python-package (main) $ hatch run test:run\n──────────────────────────────────────────────────────────────────────── test.py3.10 ────────────────────────────────────────────────────────────────────────\n==================================================================== test session starts ====================================================================\nplatform linux -- Python 3.10.16, pytest-8.4.1, pluggy-1.6.0\nUsing --randomly-seed=1490740387\nrootdir: /workspaces/pyopensci-scipy25-create-python-package\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: xdist-3.8.0, randomly-3.16.0, raises-0.11, cov-6.2.1\ncollected 2 items                                                                                                                                           \n\ntests/system/test_import.py .                                                                                                                         [ 50%]\ntests/unit/test_example.py .                                                                                                                          [100%]\n\n====================================================================== tests coverage =======================================================================\n_____________________________________________________ coverage: platform linux, python 3.10.16-final-0 ______________________________________________________\n\nName                           Stmts   Miss Branch BrPart    Cover   Missing\n----------------------------------------------------------------------------\nsrc/greatproject/__init__.py       0      0      0      0  100.00%\nsrc/greatproject/example.py        2      0      0      0  100.00%\n----------------------------------------------------------------------------\nTOTAL                              2      0      0      0  100.00%\n===================================================================== 2 passed in 0.05s =====================================================================\n──────────────────────────────────────────────────────────────────────── test.py3.11 ────────────────────────────────────────────────────────────────────────\n==================================================================== test session starts ====================================================================\nplatform linux -- Python 3.11.12, pytest-8.4.1, pluggy-1.6.0\nUsing --randomly-seed=1596865075\nrootdir: /workspaces/pyopensci-scipy25-create-python-package\nconfigfile: pyproject.toml\ntestpaths: tests\nplugins: xdist-3.8.0, randomly-3.16.0, raises-0.11, cov-6.2.1\ncollected 2 items                                                                                                                                           \n\ntests/system/test_import.py .                                                                                                                         [ 50%]\ntests/unit/test_example.py .                                                                                                                          [100%]\n\n====================================================================== tests coverage =======================================================================\n_____________________________________________________ coverage: platform linux, python 3.11.12-final-0 ______________________________________________________\n\nName                           Stmts   Miss Branch BrPart    Cover   Missing\n----------------------------------------------------------------------------\nsrc/greatproject/__init__.py       0      0      0      0  100.00%\nsrc/greatproject/example.py        2      0      0      0  100.00%\n----------------------------------------------------------------------------\nTOTAL                              2      0      0      0  100.00%\n===================================================================== 2 passed in 0.05s =====================================================================\n\n\n\nYou can build your package using the environment and scripts defined in the build tables:\nhatch run build:check\nThis script builds and checks the output distribution files of your package.\nThis build environment table declares that pip and twine should be added to that environment. Adding pip to the environment ensures that it is a current, up-to-date version.\n[tool.hatch.envs.build]\ndescription = \"\"\"Build and test your package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\ndetached = true\n# This table installs created the command hatch run install:check which will build and check your package.\n[tool.hatch.envs.install.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\nThis uses the above environment and tells hatch to run\n\npip check, # verifies your dependencies\nhatch build --clean\n\ntwine check dist/* # this checks your distribution for metadata and other potential issues. to build and test your package."
  },
  {
    "objectID": "DEVELOPMENT.html#build-your-package",
    "href": "DEVELOPMENT.html#build-your-package",
    "title": "Development Guide",
    "section": "",
    "text": "You can build your package using the environment and scripts defined in the build tables:\nhatch run build:check\nThis script builds and checks the output distribution files of your package.\nThis build environment table declares that pip and twine should be added to that environment. Adding pip to the environment ensures that it is a current, up-to-date version.\n[tool.hatch.envs.build]\ndescription = \"\"\"Build and test your package.\"\"\"\ndependencies = [\n    \"pip\",\n    \"twine\",\n]\ndetached = true\n# This table installs created the command hatch run install:check which will build and check your package.\n[tool.hatch.envs.install.scripts]\ncheck = [\n    \"pip check\",\n    \"hatch build {args:--clean}\",\n    \"twine check dist/*\",\n]\nThis uses the above environment and tells hatch to run\n\npip check, # verifies your dependencies\nhatch build --clean\n\ntwine check dist/* # this checks your distribution for metadata and other potential issues. to build and test your package."
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog, and this project adheres to Semantic Versioning.\n\n\n\nUpcoming features and fixes\n\n\n\n\n\n\n\nFunction specifications and documentation for correlation_report() (#15)\nFunction specifications and documentation for detect_anomalies() (#16)\nFunction specifications and documentation for remove_duplicates() (#21)\nFunction specifications and documentation for missing_values() (#22)\nSupport for handling missing values in categorical columns (#17)\n\n\n\n\n\nUpdated README.md with project details and usage information (#24)\nUpdated CONTRIBUTING.md with contribution guidelines (#18)\nUpdated CODE_OF_CONDUCT.md with code of conduct (#23)\n\n\n\n\n\n\n\n\nImplementation of correlation_report() for computing pairwise correlations (#44)\nImplementation of detect_anomalies() for identifying outliers in data (#42)\nImplementation of remove_duplicates() for duplicate row detection and removal (#45)\nImplementation of missing_values() for handling missing data (#49)\nUnit tests for correlation_report() (#44)\nUnit tests for detect_anomalies() (#42)\nUnit tests for remove_duplicates() (#45)\nUnit tests for missing_values() (#49)\nDependencies added to pyproject.toml (#42)\nEnvironment configuration file environment.yml (#52)\n\n\n\n\n\nUpdated README.md installation instructions for clarity (#51, #52, #53)\n\n\n\n\n\nUpdated CHANGELOG.md to reflect Milestone 1 and 2 progress (#39)\nUpdated __init__.py with correct package imports (#53)\n\n\n\n\n\n\n\n\nImplemented continuous integration for the project’s workflow. This runs the test suite and conducts style checkers on pushed and pull requests to the main branch #64\nImplemented continuous deployment for the project’s workflow. This runs the test suite, style checkers, deploys our package to Test PyPI #68\nImplemented continuous deployment to build and deploy function documentation via quartodoc #65\nAdded 4 new unit tests for correlation_report() function #66\nSet up function documentation using quartodoc #65\n\n\n\n\n\nUpdated readme with new installation instructions #70\nUpdated Changelog to reflect Milestone 3 updates #73"
  },
  {
    "objectID": "CHANGELOG.html#unreleased",
    "href": "CHANGELOG.html#unreleased",
    "title": "Changelog",
    "section": "",
    "text": "Upcoming features and fixes"
  },
  {
    "objectID": "CHANGELOG.html#milestone-1---2026-01-10",
    "href": "CHANGELOG.html#milestone-1---2026-01-10",
    "title": "Changelog",
    "section": "",
    "text": "Function specifications and documentation for correlation_report() (#15)\nFunction specifications and documentation for detect_anomalies() (#16)\nFunction specifications and documentation for remove_duplicates() (#21)\nFunction specifications and documentation for missing_values() (#22)\nSupport for handling missing values in categorical columns (#17)\n\n\n\n\n\nUpdated README.md with project details and usage information (#24)\nUpdated CONTRIBUTING.md with contribution guidelines (#18)\nUpdated CODE_OF_CONDUCT.md with code of conduct (#23)"
  },
  {
    "objectID": "CHANGELOG.html#milestone-2---2026-01-17",
    "href": "CHANGELOG.html#milestone-2---2026-01-17",
    "title": "Changelog",
    "section": "",
    "text": "Implementation of correlation_report() for computing pairwise correlations (#44)\nImplementation of detect_anomalies() for identifying outliers in data (#42)\nImplementation of remove_duplicates() for duplicate row detection and removal (#45)\nImplementation of missing_values() for handling missing data (#49)\nUnit tests for correlation_report() (#44)\nUnit tests for detect_anomalies() (#42)\nUnit tests for remove_duplicates() (#45)\nUnit tests for missing_values() (#49)\nDependencies added to pyproject.toml (#42)\nEnvironment configuration file environment.yml (#52)\n\n\n\n\n\nUpdated README.md installation instructions for clarity (#51, #52, #53)\n\n\n\n\n\nUpdated CHANGELOG.md to reflect Milestone 1 and 2 progress (#39)\nUpdated __init__.py with correct package imports (#53)"
  },
  {
    "objectID": "CHANGELOG.html#milestone-3---2026-01-24",
    "href": "CHANGELOG.html#milestone-3---2026-01-24",
    "title": "Changelog",
    "section": "",
    "text": "Implemented continuous integration for the project’s workflow. This runs the test suite and conducts style checkers on pushed and pull requests to the main branch #64\nImplemented continuous deployment for the project’s workflow. This runs the test suite, style checkers, deploys our package to Test PyPI #68\nImplemented continuous deployment to build and deploy function documentation via quartodoc #65\nAdded 4 new unit tests for correlation_report() function #66\nSet up function documentation using quartodoc #65\n\n\n\n\n\nUpdated readme with new installation instructions #70\nUpdated Changelog to reflect Milestone 3 updates #73"
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Tutorial",
    "section": "",
    "text": "This tutorial demonstrates an example workflow for how to use the data_fixr package to clean and analyze your data."
  },
  {
    "objectID": "tutorial.html#installation",
    "href": "tutorial.html#installation",
    "title": "Tutorial",
    "section": "Installation",
    "text": "Installation\nFirst, install the package from Test PyPI:\npip install -i https://test.pypi.org/simple/ data-fixr"
  },
  {
    "objectID": "tutorial.html#getting-started",
    "href": "tutorial.html#getting-started",
    "title": "Tutorial",
    "section": "Getting Started",
    "text": "Getting Started\nImport the functions you need:\n\nfrom data_fixr import (\n    correlation_report,\n    remove_duplicates,\n    detect_anomalies,\n    missing_values\n)\nimport pandas as pd"
  },
  {
    "objectID": "tutorial.html#load-sample-data",
    "href": "tutorial.html#load-sample-data",
    "title": "Tutorial",
    "section": "Load Sample Data",
    "text": "Load Sample Data\nLet’s create a sample dataset to demonstrate the package functionality:\n\n# Create sample data with some issues\ndata = pd.DataFrame({\n    'age': [25, 30, 25, 150, 35, 30, 28, 45, 32, 29],  # 150 is an anomaly\n    'income': [50000, 60000, 50000, 70000, None, 60000, 55000, 85000, 62000, 58000], #contains missing value and duplicates\n    'years_experience': [3, 7, 3, 25, 10, 7, 5, 20, 8, 6]\n})\ndata\n\n\n\n\n\n\n\n\nage\nincome\nyears_experience\n\n\n\n\n0\n25\n50000.0\n3\n\n\n1\n30\n60000.0\n7\n\n\n2\n25\n50000.0\n3\n\n\n3\n150\n70000.0\n25\n\n\n4\n35\nNaN\n10\n\n\n5\n30\n60000.0\n7\n\n\n6\n28\n55000.0\n5\n\n\n7\n45\n85000.0\n20\n\n\n8\n32\n62000.0\n8\n\n\n9\n29\n58000.0\n6"
  },
  {
    "objectID": "tutorial.html#detecting-missing-values",
    "href": "tutorial.html#detecting-missing-values",
    "title": "Tutorial",
    "section": "Detecting Missing Values",
    "text": "Detecting Missing Values\nCheck for missing values in the data and fill them in with method: ‘mode’ To see alternative method values, navigate to the reference page.\n\nmissing_report = missing_values(data, method='mode')\nmissing_report\n\n(   age   income  years_experience\n 0   25  50000.0                 3\n 1   30  60000.0                 7\n 2   25  50000.0                 3\n 3  150  70000.0                25\n 4   35  50000.0                10\n 5   30  60000.0                 7\n 6   28  55000.0                 5\n 7   45  85000.0                20\n 8   32  62000.0                 8\n 9   29  58000.0                 6,\n np.float64(3.3333333333333335))\n\n\nAs well as filling in the missing values with the specified method, the function also returns a column with the percentage of total DataFrame values that were originally missing and have been filled."
  },
  {
    "objectID": "tutorial.html#removing-duplicates",
    "href": "tutorial.html#removing-duplicates",
    "title": "Tutorial",
    "section": "Removing Duplicates",
    "text": "Removing Duplicates\nIdentify and remove duplicate rows, keeping the first instance of the duplicated observation:\n\nclean_data = remove_duplicates(data, keep='first', report=True)\nclean_data\n\n(   age   income  years_experience\n 0   25  50000.0                 3\n 1   30  60000.0                 7\n 3  150  70000.0                25\n 4   35      NaN                10\n 6   28  55000.0                 5\n 7   45  85000.0                20\n 8   32  62000.0                 8\n 9   29  58000.0                 6,\n {'total_rows': 10,\n  'duplicate_rows': 2,\n  'rows_removed': 2,\n  'strategy': 'first',\n  'cols_used': None})\n\n\nAs well as removing the duplicated observation, the function also produces a report of the the number of duplicates detected, number of rows removed, strategy used etc. To see full details of the report parameter feature, see the reference page."
  },
  {
    "objectID": "tutorial.html#detecting-anomalies",
    "href": "tutorial.html#detecting-anomalies",
    "title": "Tutorial",
    "section": "Detecting Anomalies",
    "text": "Detecting Anomalies\nFind outliers or anomalous values with default zscore method in the age and years experience columns:\n\nanomalies = detect_anomalies(data[['age', 'years_experience']])\nanomalies\n\n(   age  years_experience  age_outlier  years_experience_outlier\n 0   25                 3        False                     False\n 1   30                 7        False                     False\n 2   25                 3        False                     False\n 3  150                25         True                      True\n 4   35                10        False                     False\n 5   30                 7        False                     False\n 6   28                 5        False                     False\n 7   45                20        False                     False\n 8   32                 8        False                     False\n 9   29                 6        False                     False,\n np.float64(10.0))\n\n\nThe result returned is a tuple of a dataframe summarizing which obervations are outliers using boolean (True/False) column, along with an outlier percnetage score across all numeric columns.\nNote: We exclude the income column here because it contains missing values. The detect_anomalies function requires complete data for anomaly detection. You can handle missing values first using the missing_values function.\nThe function can also use the ‘iqr’ method for detecting anomalies, see reference page for more information."
  },
  {
    "objectID": "tutorial.html#correlation-analysis",
    "href": "tutorial.html#correlation-analysis",
    "title": "Tutorial",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nGenerate a correlation report showing pairwise correlations between all numeric variables in the data using the pearson correlation method:\n\ncorr_report = correlation_report(data, method='spearman')\ncorr_report\n\n\n\n\n\n\n\n\nfeature_1\nfeature_2\ncorrelation\nabs_correlation\n\n\n\n\n0\nage\nincome\n0.983051\n0.983051\n\n\n1\nage\nyears_experience\n1.000000\n1.000000\n\n\n2\nincome\nyears_experience\n0.983051\n0.983051\n\n\n\n\n\n\n\nEach row in the returned report represents the correlation value for each unique pair of numeric features. An absolute correlation value is also computed. Navigate to the reference page to explore alternative values for the method parameter."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing",
    "section": "",
    "text": "Contributions of all kinds are welcome for the data_fixr package and are greatly appreciated! Every little bit helps, and credit will always be given.\n\n\nThis project follows a Github Flow Workflow.\n\nThe main branch always contains stable, production-ready code\nDirect commits to the ‘main’ branch are prohibited.\nAll work should be conducted on short-lived branches created from ‘main’.\nBranches should be named using appropriate prefixes such as ‘feat-’ or ‘fix-’.\nChanges must be proposed via pull request to the main branch.\nEach Pull request must be reviewed by at least one other team member and granted approval before merging.\n\n\n\n\nYou can contribute in many ways, for example:\n\nReport bugs\nFix Bugs\nImplement Features\nWrite Documentation\nSubmit Feedback\n\n\n\nReport bugs at https://github.com/UBC-MDS/data_fixr/issues.\nIf you are reporting a bug, please follow the template guidelines. The more detailed your report, the easier and thus faster we can help you.\n\n\n\nLook through the GitHub issues for bugs. Anything labelled with bug and help wanted is open to whoever wants to implement it. When you decide to work on such an issue, please assign yourself to it and add a comment that you’ll be working on that, too. If you see another issue without the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\nLook through the GitHub issues for features. Anything labelled with enhancement and help wanted is open to whoever wants to implement it. As for fixing bugs, please assign yourself to the issue and add a comment that you’ll be working on that, too. If another enhancement catches your fancy, but it doesn’t have the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\ndata_fixr could always use more documentation, whether as part of the official documentation, in docstrings, or even on the web in blog posts, articles, and such. Just open an issue to let us know what you will be working on so that we can provide you with guidance.\n\n\n\nThe best way to send feedback is to file an issue at https://github.com/UBC-MDS/data_fixr/issues. If your feedback fits the format of one of the issue templates, please use that. Remember that this is a volunteer-driven project and everybody has limited time.\n\n\n\n\nReady to contribute? Here’s how to set up data_fixr for local development.\n\nFork the https://github.com/UBC-MDS/data_fixr repository on GitHub.\nClone your fork locally (if you want to work locally)\ngit clone git@github.com:your_name_here/data_fixr.git\nInstall hatch.\nCreate a branch for local development using the default branch (typically main) as a starting point. Use fix or feat as a prefix for your branch name.\ngit checkout main\ngit checkout -b fix-name-of-your-bugfix\nNow you can make your changes locally.\nWhen you’re done making changes, apply the quality assurance tools and check that your changes pass our test suite. This is all included with Hatch.\nhatch run test:run\nCommit your changes and push your branch to GitHub. Please use semantic commit messages.\ngit add .\ngit commit -m \"fix: summarize your changes\"\ngit push -u origin fix-name-of-your-bugfix\nOpen the link displayed in the message when pushing your new branch in order to submit a pull request.\n\n\n\n\nBefore you submit a pull request, check that it meets these guidelines:\n\nThe pull request should include tests.\nIf the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring.\nYour pull request will automatically be checked by the full test suite. It needs to pass all of them before it can be considered for merging.\n\n\n\n\nPlease note that the data_fixr package is released with a Code of Conduct. By contributing to this project you agree to abide by its terms."
  },
  {
    "objectID": "CONTRIBUTING.html#branching-and-workflow",
    "href": "CONTRIBUTING.html#branching-and-workflow",
    "title": "Contributing",
    "section": "",
    "text": "This project follows a Github Flow Workflow.\n\nThe main branch always contains stable, production-ready code\nDirect commits to the ‘main’ branch are prohibited.\nAll work should be conducted on short-lived branches created from ‘main’.\nBranches should be named using appropriate prefixes such as ‘feat-’ or ‘fix-’.\nChanges must be proposed via pull request to the main branch.\nEach Pull request must be reviewed by at least one other team member and granted approval before merging."
  },
  {
    "objectID": "CONTRIBUTING.html#example-contributions",
    "href": "CONTRIBUTING.html#example-contributions",
    "title": "Contributing",
    "section": "",
    "text": "You can contribute in many ways, for example:\n\nReport bugs\nFix Bugs\nImplement Features\nWrite Documentation\nSubmit Feedback\n\n\n\nReport bugs at https://github.com/UBC-MDS/data_fixr/issues.\nIf you are reporting a bug, please follow the template guidelines. The more detailed your report, the easier and thus faster we can help you.\n\n\n\nLook through the GitHub issues for bugs. Anything labelled with bug and help wanted is open to whoever wants to implement it. When you decide to work on such an issue, please assign yourself to it and add a comment that you’ll be working on that, too. If you see another issue without the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\nLook through the GitHub issues for features. Anything labelled with enhancement and help wanted is open to whoever wants to implement it. As for fixing bugs, please assign yourself to the issue and add a comment that you’ll be working on that, too. If another enhancement catches your fancy, but it doesn’t have the help wanted label, just post a comment, the maintainers are usually happy for any support that they can get.\n\n\n\ndata_fixr could always use more documentation, whether as part of the official documentation, in docstrings, or even on the web in blog posts, articles, and such. Just open an issue to let us know what you will be working on so that we can provide you with guidance.\n\n\n\nThe best way to send feedback is to file an issue at https://github.com/UBC-MDS/data_fixr/issues. If your feedback fits the format of one of the issue templates, please use that. Remember that this is a volunteer-driven project and everybody has limited time."
  },
  {
    "objectID": "CONTRIBUTING.html#get-started",
    "href": "CONTRIBUTING.html#get-started",
    "title": "Contributing",
    "section": "",
    "text": "Ready to contribute? Here’s how to set up data_fixr for local development.\n\nFork the https://github.com/UBC-MDS/data_fixr repository on GitHub.\nClone your fork locally (if you want to work locally)\ngit clone git@github.com:your_name_here/data_fixr.git\nInstall hatch.\nCreate a branch for local development using the default branch (typically main) as a starting point. Use fix or feat as a prefix for your branch name.\ngit checkout main\ngit checkout -b fix-name-of-your-bugfix\nNow you can make your changes locally.\nWhen you’re done making changes, apply the quality assurance tools and check that your changes pass our test suite. This is all included with Hatch.\nhatch run test:run\nCommit your changes and push your branch to GitHub. Please use semantic commit messages.\ngit add .\ngit commit -m \"fix: summarize your changes\"\ngit push -u origin fix-name-of-your-bugfix\nOpen the link displayed in the message when pushing your new branch in order to submit a pull request."
  },
  {
    "objectID": "CONTRIBUTING.html#pull-request-guidelines",
    "href": "CONTRIBUTING.html#pull-request-guidelines",
    "title": "Contributing",
    "section": "",
    "text": "Before you submit a pull request, check that it meets these guidelines:\n\nThe pull request should include tests.\nIf the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring.\nYour pull request will automatically be checked by the full test suite. It needs to pass all of them before it can be considered for merging."
  },
  {
    "objectID": "CONTRIBUTING.html#code-of-conduct",
    "href": "CONTRIBUTING.html#code-of-conduct",
    "title": "Contributing",
    "section": "",
    "text": "Please note that the data_fixr package is released with a Code of Conduct. By contributing to this project you agree to abide by its terms."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Function reference",
    "section": "",
    "text": "Functions to inspect docstrings.\n\n\n\ncorrelation_report\n\n\n\ndetect_anomalies\n\n\n\nmissing_values\n\n\n\nremove_duplicates",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/index.html#some-functions",
    "href": "reference/index.html#some-functions",
    "title": "Function reference",
    "section": "",
    "text": "Functions to inspect docstrings.\n\n\n\ncorrelation_report\n\n\n\ndetect_anomalies\n\n\n\nmissing_values\n\n\n\nremove_duplicates",
    "crumbs": [
      "Reference",
      "Function reference"
    ]
  },
  {
    "objectID": "reference/correlation_report.html",
    "href": "reference/correlation_report.html",
    "title": "correlation_report",
    "section": "",
    "text": "correlation_report\n\n\n\n\n\nName\nDescription\n\n\n\n\ncorrelation_report\nCompute pairwise correlations between numeric columns of a DataFrame.\n\n\n\n\n\ncorrelation_report.correlation_report(df, method='pearson')\nCompute pairwise correlations between numeric columns of a DataFrame.\nThis function is intended for exploratory data analysis diagnostics without plotting. It computes pairwise correlations between numeric features and returns a long-format report table, where each row corresponds to a unique feature pair.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput pandas DataFrame containing the data to analyze.\nrequired\n\n\nmethod\nstr\nCorrelation method to use. Supported values are: - “pearson”: linear correlation. - “spearman”: rank-based correlation. - “kendall”: rank-based correlation.\n'pearson'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nA long-format correlation report with the following columns: - feature_1: name of the first feature - feature_2: name of the second feature - correlation: correlation value - abs_correlation: absolute value of the correlation Each row represents a unique pair of numeric features. Self-correlations and duplicate symmetric pairs are excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the supported correlation methods. If fewer than two numeric columns are available for correlation.\n\n\n\n\n\n\n\nOnly numeric columns are considered.\nMissing values are handled according to pandas’ correlation behavior.\nThis function does not generate plots or files.\nThe output is intended to be machine-readable and suitable for use in automated analysis or reporting pipelines.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"age\": [20, 30, 40],\n...     \"income\": [40000, 60000, 80000],\n...     \"score\": [3, 2, 1],\n... })\n&gt;&gt;&gt; correlation_report(df, method=\"pearson\")\n  feature_1 feature_2  correlation  abs_correlation\n0       age    income          1.0              1.0\n1       age     score         -1.0              1.0\n2    income     score         -1.0              1.0",
    "crumbs": [
      "Reference",
      "Some functions",
      "correlation_report"
    ]
  },
  {
    "objectID": "reference/correlation_report.html#functions",
    "href": "reference/correlation_report.html#functions",
    "title": "correlation_report",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ncorrelation_report\nCompute pairwise correlations between numeric columns of a DataFrame.\n\n\n\n\n\ncorrelation_report.correlation_report(df, method='pearson')\nCompute pairwise correlations between numeric columns of a DataFrame.\nThis function is intended for exploratory data analysis diagnostics without plotting. It computes pairwise correlations between numeric features and returns a long-format report table, where each row corresponds to a unique feature pair.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\n\nInput pandas DataFrame containing the data to analyze.\nrequired\n\n\nmethod\nstr\nCorrelation method to use. Supported values are: - “pearson”: linear correlation. - “spearman”: rank-based correlation. - “kendall”: rank-based correlation.\n'pearson'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\npandas.DataFrame\nA long-format correlation report with the following columns: - feature_1: name of the first feature - feature_2: name of the second feature - correlation: correlation value - abs_correlation: absolute value of the correlation Each row represents a unique pair of numeric features. Self-correlations and duplicate symmetric pairs are excluded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nTypeError\nIf df is not a pandas DataFrame.\n\n\n\nValueError\nIf method is not one of the supported correlation methods. If fewer than two numeric columns are available for correlation.\n\n\n\n\n\n\n\nOnly numeric columns are considered.\nMissing values are handled according to pandas’ correlation behavior.\nThis function does not generate plots or files.\nThe output is intended to be machine-readable and suitable for use in automated analysis or reporting pipelines.\n\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; df = pd.DataFrame({\n...     \"age\": [20, 30, 40],\n...     \"income\": [40000, 60000, 80000],\n...     \"score\": [3, 2, 1],\n... })\n&gt;&gt;&gt; correlation_report(df, method=\"pearson\")\n  feature_1 feature_2  correlation  abs_correlation\n0       age    income          1.0              1.0\n1       age     score         -1.0              1.0\n2    income     score         -1.0              1.0",
    "crumbs": [
      "Reference",
      "Some functions",
      "correlation_report"
    ]
  },
  {
    "objectID": "reference/detect_anomalies.html",
    "href": "reference/detect_anomalies.html",
    "title": "detect_anomalies",
    "section": "",
    "text": "detect_anomalies\n\n\n\n\n\nName\nDescription\n\n\n\n\ndetect_anomalies\nThis function flags outliers in numeric columns using either the\n\n\n\n\n\ndetect_anomalies.detect_anomalies(df, method='zscore')\nThis function flags outliers in numeric columns using either the Z-score method or the IQR method.\nOutliers in a dataset can heavily impact our analysis negatively. This function helps to identify potential anomalies in numeric columns of a pandas DataFrame, whether the data is normally distributed or skewed.\nThe function takes in a DataFrame, automatically identifies numeric columns, and applies the specified method to flag outliers. Each numeric column is analyzed independently to detect anomalous values.\nThe z-score method is suitable for normally distributed data but sensitive to extreme outliers, while the IQR method is better for skewed distributions and robust to extreme values.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing numeric columns to analyze for anomalies. Non-numeric columns will be excluded from the analysis.\nrequired\n\n\nmethod\nstr\nThe anomaly detection method to use. Valid options are: - ‘zscore’ : For normally distributed data - ‘iqr’ : For skewed data or robust detection\n'zscore'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple of (pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame containing only the numeric columns plus additional boolean columns (named as ’{column}_outlier’) indicating whether each value is an outlier. True indicates an outlier, False indicates a normal value. outlier_percentage : float The percentage of outliers detected across all numeric columns, calculated as (total outliers / total values) * 100.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf method is not ‘zscore’ or ‘iqr’, or if any numeric column contains fewer than 3 data points, or if any numeric column contains NaN values.\n\n\n\nTypeError\nIf df is not a pandas DataFrame or contains no numeric columns.\n\n\n\n\n\n\nZ-score method: Identifies points that are more than 2 standard deviations away from the mean. The z-score is calculated as: z = (x - mean) / std. A data point is flagged as an outlier if |z| &gt; 2.\nIQR method: Uses the interquartile range to identify outliers. Calculates Q1 (25th percentile) and Q3 (75th percentile), then IQR = Q3 - Q1. A data point is flagged as an outlier if it falls below Q1 - 1.5×IQR or above Q3 + 1.5×IQR.\nAssumptions: - Requires at least 3 data points per numeric column for meaningful analysis - Non-numeric columns are automatically excluded - Missing values (NaN) are not flagged as outliers but will raise an error if present in numeric columns.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Create sample data with clear outliers\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'temperature': [20, 21, 22, 19, 98, 23],\n...     'humidity': [45, 50, 48, 52, 49, 200],\n...     'location': ['A', 'B', 'C', 'D', 'E', 'F']\n... })\n&gt;&gt;&gt; result_df, pct = detect_anomalies(df, method='zscore')\n&gt;&gt;&gt; print(result_df)\n   temperature  humidity  temperature_outlier  humidity_outlier\n0           20        45                False             False\n1           21        50                False             False\n2           22        48                False             False\n3           19        52                False             False\n4           98        49                 True             False\n5           23       200                False              True\n&gt;&gt;&gt; print(f\"{pct:.1f}% of data points are outliers\")\n16.7% of data points are outliers",
    "crumbs": [
      "Reference",
      "Some functions",
      "detect_anomalies"
    ]
  },
  {
    "objectID": "reference/detect_anomalies.html#functions",
    "href": "reference/detect_anomalies.html#functions",
    "title": "detect_anomalies",
    "section": "",
    "text": "Name\nDescription\n\n\n\n\ndetect_anomalies\nThis function flags outliers in numeric columns using either the\n\n\n\n\n\ndetect_anomalies.detect_anomalies(df, method='zscore')\nThis function flags outliers in numeric columns using either the Z-score method or the IQR method.\nOutliers in a dataset can heavily impact our analysis negatively. This function helps to identify potential anomalies in numeric columns of a pandas DataFrame, whether the data is normally distributed or skewed.\nThe function takes in a DataFrame, automatically identifies numeric columns, and applies the specified method to flag outliers. Each numeric column is analyzed independently to detect anomalous values.\nThe z-score method is suitable for normally distributed data but sensitive to extreme outliers, while the IQR method is better for skewed distributions and robust to extreme values.\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\nDefault\n\n\n\n\ndf\npd.DataFrame\nThe DataFrame containing numeric columns to analyze for anomalies. Non-numeric columns will be excluded from the analysis.\nrequired\n\n\nmethod\nstr\nThe anomaly detection method to use. Valid options are: - ‘zscore’ : For normally distributed data - ‘iqr’ : For skewed data or robust detection\n'zscore'\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\ntuple of (pd.DataFrame, float)\nresult_df : pd.DataFrame A DataFrame containing only the numeric columns plus additional boolean columns (named as ’{column}_outlier’) indicating whether each value is an outlier. True indicates an outlier, False indicates a normal value. outlier_percentage : float The percentage of outliers detected across all numeric columns, calculated as (total outliers / total values) * 100.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nName\nType\nDescription\n\n\n\n\n\nValueError\nIf method is not ‘zscore’ or ‘iqr’, or if any numeric column contains fewer than 3 data points, or if any numeric column contains NaN values.\n\n\n\nTypeError\nIf df is not a pandas DataFrame or contains no numeric columns.\n\n\n\n\n\n\nZ-score method: Identifies points that are more than 2 standard deviations away from the mean. The z-score is calculated as: z = (x - mean) / std. A data point is flagged as an outlier if |z| &gt; 2.\nIQR method: Uses the interquartile range to identify outliers. Calculates Q1 (25th percentile) and Q3 (75th percentile), then IQR = Q3 - Q1. A data point is flagged as an outlier if it falls below Q1 - 1.5×IQR or above Q3 + 1.5×IQR.\nAssumptions: - Requires at least 3 data points per numeric column for meaningful analysis - Non-numeric columns are automatically excluded - Missing values (NaN) are not flagged as outliers but will raise an error if present in numeric columns.\n\n\n\n&gt;&gt;&gt; import pandas as pd\n&gt;&gt;&gt; # Create sample data with clear outliers\n&gt;&gt;&gt; df = pd.DataFrame({\n...     'temperature': [20, 21, 22, 19, 98, 23],\n...     'humidity': [45, 50, 48, 52, 49, 200],\n...     'location': ['A', 'B', 'C', 'D', 'E', 'F']\n... })\n&gt;&gt;&gt; result_df, pct = detect_anomalies(df, method='zscore')\n&gt;&gt;&gt; print(result_df)\n   temperature  humidity  temperature_outlier  humidity_outlier\n0           20        45                False             False\n1           21        50                False             False\n2           22        48                False             False\n3           19        52                False             False\n4           98        49                 True             False\n5           23       200                False              True\n&gt;&gt;&gt; print(f\"{pct:.1f}% of data points are outliers\")\n16.7% of data points are outliers",
    "crumbs": [
      "Reference",
      "Some functions",
      "detect_anomalies"
    ]
  },
  {
    "objectID": "docs/index.html",
    "href": "docs/index.html",
    "title": "Welcome to data_fixr’s Documentation ’",
    "section": "",
    "text": ":maxdepth: 2 :hidden: :caption: Contents:\nHome \n\nThis is the landing page of your docs. you can update it as you’d like to. This documentation example uses myst markdown as the primary documentation syntax.\n:::{button-link} https://www.pyopensci.org/python-package-guide/documentation/hosting-tools/myst-markdown-rst-doc-syntax.html :color: primary :class: sd-rounded-pill float-left\nLearn more about myst in our pyOpenSci packaging guide.\n:::\nMyst is a version of markdown that has more formatting flexibility. This is what a sphinx directive looks like using myst markdown formatting:\n:::{toctree}\n:maxdepth: 2\n:caption: Contents:\n:::\nIf you see syntax like the syntax below, you are looking at rst.\n.. toctree::\n   :maxdepth: 2\n   :caption: Contents:\n\n\n\n\nCopyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe .\nFree software distributed under the MIT License."
  },
  {
    "objectID": "docs/index.html#overview",
    "href": "docs/index.html#overview",
    "title": "Welcome to data_fixr’s Documentation ’",
    "section": "",
    "text": ":maxdepth: 2 :hidden: :caption: Contents:\nHome \n\nThis is the landing page of your docs. you can update it as you’d like to. This documentation example uses myst markdown as the primary documentation syntax.\n:::{button-link} https://www.pyopensci.org/python-package-guide/documentation/hosting-tools/myst-markdown-rst-doc-syntax.html :color: primary :class: sd-rounded-pill float-left\nLearn more about myst in our pyOpenSci packaging guide.\n:::\nMyst is a version of markdown that has more formatting flexibility. This is what a sphinx directive looks like using myst markdown formatting:\n:::{toctree}\n:maxdepth: 2\n:caption: Contents:\n:::\nIf you see syntax like the syntax below, you are looking at rst.\n.. toctree::\n   :maxdepth: 2\n   :caption: Contents:"
  },
  {
    "objectID": "docs/index.html#copyright",
    "href": "docs/index.html#copyright",
    "title": "Welcome to data_fixr’s Documentation ’",
    "section": "",
    "text": "Copyright © 2026 Nour Shawky, Apoorva Srivastava, Zain Nofal, Chikire Aku-Ibe .\nFree software distributed under the MIT License."
  }
]